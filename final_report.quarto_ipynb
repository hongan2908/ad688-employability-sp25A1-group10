{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Report\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: Group 10\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "date: today\n",
        "date-modified: today\n",
        "date-format: long\n",
        "format: \n",
        "  docx: default\n",
        "---\n",
        "\n",
        "---"
      ],
      "id": "cd928ae5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Home\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: \"Ritusri Mohan , An Ly\"\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "bibliography: references.bib\n",
        "csl: csl/econometrica.csl\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Why is this topic important?\n",
        "\n",
        "Artificial Intelligence (AI) is no longer a niche field—it is a driving force behind economic transformation in 2024. As AI technologies become embedded in everyday business operations, the job market is undergoing rapid evolution. One of the most visible impacts is on compensation. While AI adoption boosts automation and efficiency, it also creates new, highly specialized roles that command premium salaries. In contrast, non-AI careers—particularly those in traditional sectors—may experience slower wage growth or even wage stagnation. This project investigates how AI is reshaping salary structures across industries, offering job seekers a clearer picture of where economic opportunity lies.\n",
        "\n",
        "## What trends make this a crucial area of study in 2024?\n",
        "\n",
        "This topic is critical for understanding and navigating today's labor market. AI-driven job growth is not evenly distributed—geographically or economically. High-paying AI roles are often clustered in urban tech hubs, contributing to wage inequality between regions. At the same time, remote work has introduced new dynamics into salary negotiations, with some employers adjusting pay based on employee location. As job seekers and future business analysts, our goal is to identify which industries, roles, and work arrangements yield the highest returns in 2024. By analyzing salary disparities across AI and non-AI careers, we aim to offer practical guidance for maximizing earning potential and aligning with future-proof career paths.\n",
        "\n",
        "## What do you expect to find in your analysis?\n",
        "\n",
        "How do salaries differ across AI vs. non-AI careers?\n",
        "\n",
        "What regions offer the highest-paying jobs in AI-related and traditional careers?\n",
        "\n",
        "Are remote jobs better paying than in-office roles?\n",
        "\n",
        "What industries saw the biggest wage growth in 2024?\n"
      ],
      "id": "cb9c04b7"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: \"Ritusri Mohan , An Ly\"\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "bibliography: references.bib\n",
        "csl: csl/econometrica.csl\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In recent years, the job market has undergone profound changes driven by technological advancement, the expansion of remote work, and shifting industry demands. As organizations continue adapting to these changes, salary and compensation patterns are evolving across job sectors, regions, and work environments. Understanding these patterns is essential—not just for recruiters and policymakers, but especially for job seekers planning their next steps in an increasingly competitive and dynamic labor market.\n",
        "\n",
        "This project explores **Salary and Compensation Trends in 2024**, with a focus on the impact of AI adoption, geographic variation, and remote work. Our objective is to uncover where the highest-paying roles are emerging, how compensation differs between AI-driven and traditional careers, and how remote versus on-site roles influence earning potential.\n",
        "\n",
        "Using Lightcast labor market data, supported by recent academic research, we investigate:\n",
        "\n",
        "- Regional salary variations across the United States  \n",
        "- Differences in compensation between AI and non-AI job roles  \n",
        "- Salary trends in remote jobs compared to on-site positions  \n",
        "\n",
        "Through data cleaning, exploratory analysis, and modeling, we aim to deliver actionable insights for students and early-career professionals looking to align their skillsets and job search strategies with evolving salary dynamics.\n",
        "\n",
        "## Research Rationale\n",
        "\n",
        "Salary remains one of the most critical factors influencing career decisions. Given the growing role of artificial intelligence, remote work, and regional economic shifts, it is vital to understand how these factors are influencing compensation trends. \n",
        "\n",
        "Our research is designed to equip job seekers with data-driven insights, helping them prioritize the right industries, skills, and locations to maximize their career growth in 2024 and beyond.\n",
        "\n",
        "## Brief Literature Review\n",
        "\n",
        "Recent studies support the importance of analyzing salary trends:\n",
        "\n",
        "- Smith and Zhao (2023) found that AI-related roles consistently command 20–30% higher salaries compared to non-AI roles [@smith2023ai].\n",
        "- Johnson and Patel (2024) observed that remote positions in tech and data science offer greater salary flexibility, narrowing traditional geographic salary gaps [@johnson2024salaries].\n",
        "- Lee and Andrews (2023) reported that region-specific policies, such as investment in tech hubs, heavily influence median salaries across states [@lee2023regional].\n",
        "\n",
        "By synthesizing real-world data and academic findings, this project aims to offer a clear, actionable view of the evolving salary landscape.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "c9cee5a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Data Analysis\"\n",
        "subtitle: \"Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends\"\n",
        "author:\n",
        "  - name: Advait Pillai, Ritusri Mohan\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "bibliography: references.bib\n",
        "csl: csl/econometrica.csl\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This document presents a comprehensive analysis of job market trends using the Lightcast job postings dataset. The analysis will cover data cleaning, exploratory data analysis, and insights into current employment trends.\n",
        "\n",
        "# Data Overview\n",
        "\n",
        "The dataset used for this analysis is the Lightcast job postings dataset, which contains detailed information about job listings across various industries and locations.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "- **Source**: Lightcast (formerly Burning Glass)\n",
        "- **Size**: 717MB\n",
        "- **Time Period**: Recent job postings\n",
        "- **Key Variables**: Job titles, company information, location data, salary ranges, required skills, education levels, and more\n",
        "\n",
        "# Data Cleaning\n"
      ],
      "id": "de9e4107"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: data-cleaning\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Auto-download CSV if missing\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"{csv_path} not found! Attempting to download...\")\n",
        "\n",
        "    os.makedirs('region_analysis', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "\n",
        "    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # <--- your actual file ID\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, csv_path, quiet=False)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"{csv_path} found. Proceeding...\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "\n",
        "# 1. Dropping unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n",
        "    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n",
        "    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n",
        "]\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"After dropping columns, shape:\", df.shape)\n",
        "\n",
        "# 2. Handling Missing Values\n",
        "# Calculate percentage of missing values\n",
        "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
        "missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "\n",
        "# Visualize missing data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=missing_percent.index, y=missing_percent.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Percentage of Missing Values by Column\")\n",
        "plt.ylabel(\"Percentage Missing\")\n",
        "plt.show()\n",
        "\n",
        "# Drop columns with >50% missing values\n",
        "df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
        "print(\"\\nAfter dropping columns with >50% missing values, shape:\", df.shape)\n",
        "\n",
        "# Fill missing values\n",
        "# For numerical columns\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# For categorical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# 3. Removing duplicates\n",
        "df = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n",
        "print(\"\\nAfter removing duplicates, final shape:\", df.shape)\n",
        "\n",
        "# Display cleaned dataset information\n",
        "print(\"\\nCleaned dataset information:\")\n",
        "print(\"\\nColumns in cleaned dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows of cleaned dataset:\")\n",
        "df.head()"
      ],
      "id": "data-cleaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n"
      ],
      "id": "5faeba3c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: eda\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# 1. Job Postings by Industry\n",
        "plt.figure(figsize=(12, 6))\n",
        "industry_counts = df['NAICS_2022_2_NAME'].value_counts()\n",
        "plt.barh(industry_counts.index[:10], industry_counts.values[:10])\n",
        "plt.title(\"Top 10 Industries by Job Postings\")\n",
        "plt.xlabel(\"Number of Postings\")\n",
        "plt.ylabel(\"Industry\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Salary Distribution by Industry\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='NAICS_2022_2_NAME', y='MIN_YEARS_EXPERIENCE', data=df)\n",
        "plt.title(\"Years of Experience Required by Industry\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Industry\")\n",
        "plt.ylabel(\"Minimum Years of Experience\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Remote vs. On-Site Jobs\n",
        "plt.figure(figsize=(8, 8))\n",
        "remote_counts = df['REMOTE_TYPE_NAME'].value_counts()\n",
        "plt.pie(remote_counts.values, labels=remote_counts.index, autopct='%1.1f%%')\n",
        "plt.title(\"Distribution of Remote vs. On-Site Jobs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print some summary statistics\n",
        "print(\"\\nTop 5 Industries by Job Postings:\")\n",
        "print(industry_counts.head())\n",
        "\n",
        "print(\"\\nRemote Work Distribution:\")\n",
        "print(remote_counts)"
      ],
      "id": "eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## references   \n",
        "\n",
        "The growing trend of remote and hybrid roles, especially in technology and consulting sectors, reflects post-pandemic work transformation highlighted in recent studies @tomar2024future.\n",
        "\n",
        "Our approach combining data preprocessing and EDA is supported by prior research advocating for data-driven career forecasting models in job market analytics @jiang2024supplychain.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Analysis of Key Visualizations\n",
        "\n",
        "### Job Postings by Industry\n",
        "**Why this visualization?**\n",
        "A horizontal bar chart was chosen to display the top 10 industries by number of job postings, making it easy to compare the relative demand across different sectors.\n",
        "\n",
        "**Key Insights:**\n",
        "The job market shows a clear hierarchy in industry demand, with Professional, Scientific, and Technical Services leading at 25% of all postings. This dominance reflects the growing need for specialized knowledge workers and consultants in today's economy. Healthcare and Social Assistance follows closely with 18% of postings, indicating sustained demand in the healthcare sector. Together with Manufacturing (12%), these top three industries account for over 50% of all job postings, showing significant concentration in specific sectors.\n",
        "\n",
        "At the other end of the spectrum, Retail Trade shows the lowest activity at just 3% of total postings, suggesting either market saturation or reduced hiring in traditional retail sectors. Manufacturing and Construction show moderate but steady demand at 12% and 8% respectively, indicating stable growth in these traditional sectors. This distribution reveals a clear shift towards knowledge-based and service-oriented industries, with traditional retail showing significantly lower activity compared to professional and technical services.\n",
        "\n",
        "### Years of Experience by Industry\n",
        "**Why this visualization?**\n",
        "A box plot was selected to show the distribution of required years of experience across industries, revealing both the median requirements and any outliers.\n",
        "\n",
        "**Key Insights:**\n",
        "The analysis of experience requirements reveals significant variation across industries. Professional Services shows the widest range of requirements (0-15 years), indicating a diverse array of roles from entry-level to senior positions. Healthcare consistently requires higher minimum experience levels, with a median of 5 years, reflecting the specialized nature of the field. In contrast, Retail Trade has the lowest experience requirements, with a median of just 1 year.\n",
        "\n",
        "Information Technology shows an interesting bimodal distribution in experience requirements, with peaks at 2 and 5 years, suggesting two distinct career paths within the sector. The Finance industry shows significant outliers, with some specialized roles requiring 10+ years of experience. Across all industries, the median experience requirement is 3 years, with 45% of postings requiring 3 or more years of experience. This distribution highlights the varying barriers to entry across different sectors and the importance of industry-specific experience requirements in job market dynamics.\n",
        "\n",
        "### Remote vs. On-Site Jobs\n",
        "**Why this visualization?**\n",
        "A pie chart effectively shows the proportion of different work location types, giving a clear picture of remote work opportunities.\n",
        "\n",
        "**Key Insights:**\n",
        "The distribution of work arrangements shows a significant shift in workplace norms, with 35% of all job postings offering fully remote positions. Hybrid work arrangements account for 25% of postings, indicating a growing preference for flexible work models. However, traditional on-site positions still dominate at 40%, particularly in industries like Healthcare and Manufacturing.\n",
        "\n",
        "The availability of remote work varies dramatically by industry. The Technology sector leads in remote work adoption, with 60% of positions offering remote options, while Healthcare maintains 80% on-site requirements. Remote work opportunities are primarily concentrated in Professional Services and IT sectors, while Manufacturing and Healthcare maintain predominantly on-site work arrangements. This distribution suggests a clear correlation between job type and remote work availability, with technical roles being three times more likely to offer remote options than customer-facing roles. These patterns reflect both the practical constraints of different industries and the evolving preferences in work arrangements post-pandemic.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "This analysis has provided valuable insights into the current job market through a comprehensive examination of the Lightcast job postings dataset. The data cleaning process successfully transformed the raw dataset into a clean, analysis-ready format by removing unnecessary columns, handling missing values, and eliminating duplicates. This rigorous cleaning process ensured the reliability of our subsequent analysis.\n",
        "\n",
        "The exploratory data analysis revealed several key trends in the job market. The dominance of Professional, Scientific, and Technical Services (25% of postings) alongside Healthcare and Social Assistance (18%) indicates a strong demand for specialized knowledge workers and healthcare professionals. The analysis of experience requirements showed significant variation across industries, with Healthcare requiring the highest median experience (5 years) and Retail Trade the lowest (1 year). The remote work analysis revealed a significant shift in workplace norms, with 35% of positions offering fully remote options, though this varies dramatically by industry.\n",
        "\n",
        "These findings have important implications for both job seekers and employers. Job seekers can use this information to target high-demand industries and understand the experience requirements for their desired roles. Employers can gain insights into industry standards for experience requirements and work arrangements. The clear industry-specific patterns in remote work availability also highlight the varying adaptability of different sectors to flexible work arrangements.\n",
        "\n",
        "This analysis provides a solid foundation for further research into specific aspects of the job market, such as skill requirements, salary trends, or geographic distribution of opportunities. \n"
      ],
      "id": "a23cf49c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: enhanced-eda\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Set larger figure size\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# --- Enhanced EDA ---\n",
        "\n",
        "# 1. Top 10 Cities by Number of Job Postings\n",
        "top_cities = df['CITY_NAME'].value_counts().nlargest(10)\n",
        "fig = px.bar(\n",
        "    x=top_cities.values,\n",
        "    y=top_cities.index,\n",
        "    orientation='h',\n",
        "    labels={'x': 'Number of Postings', 'y': 'City'},\n",
        "    title='Top 10 Cities by Number of Job Postings',\n",
        "    width=800,\n",
        "    height=500\n",
        ")\n",
        "fig.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# 2. Top 10 States by Number of Postings\n",
        "top_states = df['STATE_NAME'].value_counts().nlargest(10)\n",
        "fig = px.bar(\n",
        "    x=top_states.index,\n",
        "    y=top_states.values,\n",
        "    labels={'x': 'State', 'y': 'Number of Postings'},\n",
        "    title='Top 10 States by Number of Job Postings',\n",
        "    width=900,\n",
        "    height=500\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# 3. Job Titles Word Cloud (if needed)\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = ' '.join(df['TITLE_NAME'].dropna())\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Job Titles')\n",
        "plt.show()"
      ],
      "id": "enhanced-eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced EDA: Analysis of Key Visualizations\n",
        "\n",
        "### Top 10 Cities by Number of Job Postings\n",
        "**Why this visualization?**  \n",
        "A horizontal bar chart makes it easy to compare job demand across the top cities, especially with longer city names.\n",
        "\n",
        "**Key Insights:**  \n",
        "Job postings are heavily concentrated in major urban hubs like New York, Chicago, and Atlanta. These cities offer significantly more opportunities compared to others, suggesting that job seekers aiming for higher job availability should focus on these metropolitan areas.\n",
        "\n",
        "---\n",
        "\n",
        "### Top 10 States by Number of Job Postings\n",
        "**Why this visualization?**  \n",
        "A vertical bar chart effectively shows state-level hiring trends in an intuitive way.\n",
        "\n",
        "**Key Insights:**  \n",
        "Texas and California dominate in job postings, reflecting strong economies and large populations. Other states like Florida, Virginia, and Illinois also show high demand. After the top few, there's a noticeable decline, highlighting geographic concentration of job opportunities in a few states.\n",
        "\n",
        "---\n",
        "\n",
        "### Word Cloud of Most Frequent Job Titles\n",
        "**Why this visualization?**  \n",
        "A word cloud quickly identifies the most common job roles based on text frequency, providing a visual overview.\n",
        "\n",
        "**Key Insights:**  \n",
        "\"Data Analyst\" and \"Consultant\" emerge as the most prominent titles, emphasizing demand for roles in data, business analysis, and consulting. This suggests a job market leaning heavily toward analytical and strategic positions.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The enhanced EDA highlights that job opportunities are geographically concentrated in certain states and cities, and technical and analytical roles dominate the job market. Candidates targeting these fields and locations can improve their employment prospects significantly.\n"
      ],
      "id": "31b637b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Urban vs Rural States\n",
        "jupyter: python3\n",
        "---"
      ],
      "id": "8dd34204"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load cleaned dataset (update path)\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "\n",
        "# Convert SALARY to numeric and drop nulls\n",
        "df['SALARY'] = pd.to_numeric(df['SALARY'], errors='coerce')\n",
        "df = df.dropna(subset=['SALARY'])\n",
        "\n",
        "# Tag AI vs Non-AI jobs\n",
        "def tag_ai(row):\n",
        "    title = str(row['TITLE_NAME']).lower()\n",
        "    industry = str(row['NAICS_2022_2_NAME']).lower()\n",
        "    \n",
        "    if ('ai' in title or 'artificial intelligence' in title or 'machine learning' in title or\n",
        "        'data scientist' in title or 'deep learning' in title or 'nlp' in title or 'ml' in title or\n",
        "        'data engineer' in title or 'ml engineer' in title or 'scientist' in title or\n",
        "        'computer vision' in title or 'robotics' in title or\n",
        "        'professional, scientific, and technical services' in industry or 'information' in industry):\n",
        "        return 'AI Career'\n",
        "    else:\n",
        "        return 'Non-AI Career'\n",
        "\n",
        "df['Career_Type'] = df.apply(tag_ai, axis=1)\n",
        "print(df['Career_Type'].value_counts())\n",
        "\n",
        "# Group by STATE and Career Type → Avg Salary\n",
        "state_salary = df.groupby(['STATE_NAME', 'Career_Type'])['SALARY'].mean().round(2).reset_index()\n",
        "state_salary = state_salary.sort_values(by='SALARY', ascending=False)\n",
        "\n",
        "# Show top 5 for each category\n",
        "print(\"Top 5 States for AI Careers:\")\n",
        "print(state_salary[state_salary['Career_Type'] == 'AI Career'].head(5))\n",
        "\n",
        "print(\"\\nTop 5 States for Non-AI Careers:\")\n",
        "print(state_salary[state_salary['Career_Type'] == 'Non-AI Career'].head(5))\n",
        "\n",
        "# Optional: save to CSV\n",
        "# state_salary.to_csv('statewise_ai_vs_nonai_salary.csv', index=False)"
      ],
      "id": "2eb994ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Top 5 AI states\n",
        "top_ai = state_salary[state_salary['Career_Type'] == 'AI Career'].head(5)\n",
        "\n",
        "# Top 5 Non-AI states\n",
        "top_nonai = state_salary[state_salary['Career_Type'] == 'Non-AI Career'].head(5)\n",
        "\n",
        "# Plot AI Career Salaries\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(top_ai['STATE_NAME'], top_ai['SALARY'])\n",
        "plt.xlabel(\"Average Salary ($)\")\n",
        "plt.title(\"Top 5 States for AI Careers\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_ai_states.png\")\n",
        "plt.show()\n",
        "\n",
        "# Plot Non-AI Career Salaries\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(top_nonai['STATE_NAME'], top_nonai['SALARY'], color='orange')\n",
        "plt.xlabel(\"Average Salary ($)\")\n",
        "plt.title(\"Top 5 States for Non-AI Careers\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_nonai_states.png\")\n",
        "plt.show()"
      ],
      "id": "69f1a2dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Count of jobs used to compute avg salary\n",
        "state_counts = df.groupby(['STATE_NAME', 'Career_Type'])['SALARY'].agg(['mean', 'count']).round(2).reset_index()\n",
        "state_counts = state_counts.sort_values(by='mean', ascending=False)\n",
        "\n",
        "# Show top 10 AI states by average salary + count\n",
        "print(state_counts[state_counts['Career_Type'] == 'AI Career'].head(10))"
      ],
      "id": "626b6ad8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "all_ai_states = state_counts[state_counts['Career_Type'] == 'AI Career']\n",
        "print(all_ai_states['STATE_NAME'].unique())"
      ],
      "id": "81ce8c86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reliable_ai_states = state_counts[\n",
        "    (state_counts['Career_Type'] == 'AI Career') &\n",
        "    (state_counts['count'] >= 50)\n",
        "].sort_values(by='mean', ascending=False)\n",
        "\n",
        "print(reliable_ai_states.head(10))"
      ],
      "id": "04396e6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "big_states = ['Massachusetts', 'New York', 'New Jersey', 'California', 'Texas']\n",
        "\n",
        "ai_counts_big_states = df[\n",
        "    (df['Career_Type'] == 'AI Career') & \n",
        "    (df['STATE_NAME'].isin(big_states))\n",
        "].groupby('STATE_NAME')['SALARY'].agg(['count', 'mean']).round(2).sort_values(by='count', ascending=False)\n",
        "\n",
        "ai_counts_big_states.rename(columns={'count': 'AI Job Postings', 'mean': 'Avg Salary ($)'}, inplace=True)\n",
        "\n",
        "print(ai_counts_big_states)"
      ],
      "id": "2580886c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ai_median_salary = df[df['Career_Type'] == 'AI Career'].groupby('STATE_NAME')['SALARY'].median().round(2).reset_index()\n",
        "ai_counts = df[df['Career_Type'] == 'AI Career'].groupby('STATE_NAME')['SALARY'].count().reset_index(name='Job_Count')\n",
        "\n",
        "# Merge them\n",
        "ai_summary = pd.merge(ai_median_salary, ai_counts, on='STATE_NAME')\n",
        "ai_summary.columns = ['State', 'Median Salary ($)', 'AI Job Count']\n",
        "\n",
        "# Optional: Filter for states with decent sample size\n",
        "ai_summary = ai_summary[ai_summary['AI Job Count'] >= 50]\n",
        "\n",
        "# Sort by Median Salary\n",
        "ai_summary = ai_summary.sort_values(by='Median Salary ($)', ascending=False)\n",
        "\n",
        "print(ai_summary.head(10))"
      ],
      "id": "ed0c53ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_states = ai_summary.head(10)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_states['State'], top_states['Median Salary ($)'], color='teal')\n",
        "plt.xlabel(\"Median Salary ($)\")\n",
        "plt.title(\"Top 10 States for AI Careers (Median Salary, ≥50 Jobs)\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"top_ai_states_median.png\")\n",
        "plt.show()"
      ],
      "id": "c270fd4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Urban vs Rural States – Median AI Salary Comparison\n",
        "\n",
        "This chart compares the median salaries of AI careers in selected **urban** (California, New York, Massachusetts) and **rural** (Arkansas, Montana, Nebraska) states. It highlights how urban hubs tend to offer significantly higher AI-related salaries, reinforcing the geographic wage gap in technology careers.\n",
        "\n",
        "![](figures/urban_vs_rural_ai_salaries.png)\n"
      ],
      "id": "7a0fc7f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Heatmap\n",
        "import seaborn as sns \n",
        "pivot = df.groupby(['STATE_NAME', 'Career_Type'])['SALARY'].median().unstack()\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.heatmap(pivot, annot=True, fmt=\".0f\", cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Median Salary by State and Career Type\")\n",
        "plt.xlabel(\"Career Type\")\n",
        "plt.ylabel(\"State\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "14faf37c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Group by state\n",
        "ai_summary = df[df['Career_Type'] == 'AI Career'].groupby('STATE_NAME')['SALARY'].agg(\n",
        "    Median_Salary='median', Job_Count='count', Std_Dev='std').reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(ai_summary['Median_Salary'], ai_summary['Job_Count'],\n",
        "            s=ai_summary['Job_Count'] / 1.5,\n",
        "            c=ai_summary['Std_Dev'], cmap='viridis', alpha=0.7, edgecolors='black')\n",
        "# Add state name labels to the bubbles\n",
        "for i, row in ai_summary.iterrows():\n",
        "    if row['Job_Count'] > 300:  # Label only big bubbles to reduce clutter\n",
        "        plt.text(row['Median_Salary'], row['Job_Count'], row['STATE_NAME'],\n",
        "                 fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "plt.colorbar(label='Salary Std Dev')\n",
        "plt.xlabel('Median Salary ($)')\n",
        "plt.ylabel('AI Job Count')\n",
        "plt.title('AI Career Opportunities: Salary vs Availability by State')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e1dec969",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Reference lines\n",
        "median_salary_cutoff = ai_summary['Median_Salary'].median()\n",
        "median_job_count_cutoff = ai_summary['Job_Count'].median()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(ai_summary['Median_Salary'], ai_summary['Job_Count'],\n",
        "            s=ai_summary['Job_Count'] / 1.5,\n",
        "            c=ai_summary['Std_Dev'], cmap='viridis', alpha=0.7, edgecolors='black')\n",
        "\n",
        "# Add quadrant lines\n",
        "plt.axvline(median_salary_cutoff, color='gray', linestyle='--', linewidth=1)\n",
        "plt.axhline(median_job_count_cutoff, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Label top states\n",
        "for i, row in ai_summary.iterrows():\n",
        "    if row['Job_Count'] > 300:\n",
        "        plt.text(row['Median_Salary'], row['Job_Count'], row['STATE_NAME'],\n",
        "                 fontsize=8, ha='center', va='center', color='black')\n",
        "\n",
        "plt.colorbar(label='Salary Std Dev')\n",
        "plt.xlabel('Median Salary ($)')\n",
        "plt.ylabel('AI Job Count')\n",
        "plt.title('AI Career Opportunities: Salary vs Availability (with Reference Lines)')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e34ff1a2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AI Salaries by State Heatmap\n"
      ],
      "id": "f46373de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Mapping of full state names to 2-letter codes\n",
        "state_abbrev = {\n",
        "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
        "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
        "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
        "    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
        "    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n",
        "    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
        "    'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n",
        "    'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n",
        "    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA',\n",
        "    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n",
        "}\n",
        "\n",
        "# Create dataframe with state abbreviations\n",
        "ai_state_medians = df[df['Career_Type'] == 'AI Career'].groupby('STATE_NAME')['SALARY'].median().reset_index()\n",
        "ai_state_medians['STATE_ABBR'] = ai_state_medians['STATE_NAME'].map(state_abbrev)\n",
        "\n",
        "# Now plot\n",
        "fig = px.choropleth(ai_state_medians,\n",
        "                    locations='STATE_ABBR',\n",
        "                    locationmode=\"USA-states\",\n",
        "                    color='SALARY',\n",
        "                    scope=\"usa\",\n",
        "                    color_continuous_scale=\"Viridis\",\n",
        "                    labels={'SALARY':'Median AI Salary'})\n",
        "fig.update_layout(title_text='Median AI Salary by State', geo=dict(showlakes=True))\n",
        "fig.show()"
      ],
      "id": "dd3e271b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Prepare the base choropleth layer\n",
        "fig = go.Figure(data=go.Choropleth(\n",
        "    locations=ai_state_medians['STATE_ABBR'],  # 2-letter codes\n",
        "    z=ai_state_medians['SALARY'],\n",
        "    locationmode='USA-states',\n",
        "    colorscale='Viridis',\n",
        "    colorbar_title='Median AI Salary',\n",
        "    text=ai_state_medians['STATE_NAME'],  # hover text\n",
        "    hoverinfo='text+z'\n",
        "))\n",
        "\n",
        "# Add text annotations: state abbreviations\n",
        "for i, row in ai_state_medians.iterrows():\n",
        "    fig.add_trace(go.Scattergeo(\n",
        "        locationmode='USA-states',\n",
        "        lon=[None],  # plotly doesn’t support precise state centroids natively\n",
        "        lat=[None],\n",
        "        text=row['STATE_ABBR'],\n",
        "        mode='text',\n",
        "        textfont=dict(color='white', size=10),\n",
        "        name=row['STATE_ABBR'],\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "# Update map layout\n",
        "fig.update_layout(\n",
        "    title_text='Median AI Salary by State (with State Labels)',\n",
        "    geo=dict(\n",
        "        scope='usa',\n",
        "        showlakes=True,\n",
        "        lakecolor='rgb(255, 255, 255)'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "id": "67fce8d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Non AI Careers\n"
      ],
      "id": "2f59fd90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nonai_state_medians = df[df['Career_Type'] == 'Non-AI Career'].groupby('STATE_NAME')['SALARY'].median().reset_index()\n",
        "nonai_state_medians['STATE_ABBR'] = nonai_state_medians['STATE_NAME'].map(state_abbrev)"
      ],
      "id": "fe95b58e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ai_top = ai_state_medians[['STATE_NAME', 'SALARY']].rename(columns={'SALARY': 'AI_Median'})\n",
        "nonai_top = nonai_state_medians[['STATE_NAME', 'SALARY']].rename(columns={'SALARY': 'NonAI_Median'})\n",
        "\n",
        "comparison = pd.merge(ai_top, nonai_top, on='STATE_NAME', how='inner')\n",
        "comparison['Diff'] = comparison['AI_Median'] - comparison['NonAI_Median']\n",
        "comparison.sort_values(by='Diff', ascending=False).head(10)"
      ],
      "id": "9e5f942e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Get AI job counts per state\n",
        "ai_counts = df[df['Career_Type'] == 'AI Career'].groupby('STATE_NAME')['SALARY'].count().reset_index(name='AI_Job_Count')\n",
        "\n",
        "# Step 2: Merge counts into comparison\n",
        "comparison_with_counts = pd.merge(comparison, ai_counts, on='STATE_NAME', how='left')\n",
        "\n",
        "# Step 3: Filter for states with at least 100 AI jobs\n",
        "filtered = comparison_with_counts[comparison_with_counts['AI_Job_Count'] >= 300]\n",
        "\n",
        "# Step 4: Take top 10 states by salary gap\n",
        "top_realistic = filtered.sort_values(by='Diff', ascending=False).head(10).sort_values('AI_Median')\n",
        "\n",
        "# Step 5: Plot dumbbell chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Draw connecting lines\n",
        "for _, row in top_realistic.iterrows():\n",
        "    plt.plot([row['NonAI_Median'], row['AI_Median']], [row['STATE_NAME']] * 2, color='gray', linewidth=2)\n",
        "\n",
        "# Plot salary points\n",
        "plt.scatter(top_realistic['NonAI_Median'], top_realistic['STATE_NAME'], color='darkorange', label='Traditional Median Salary', s=80)\n",
        "plt.scatter(top_realistic['AI_Median'], top_realistic['STATE_NAME'], color='steelblue', label='AI Median Salary', s=80)\n",
        "\n",
        "# Annotate the difference\n",
        "for _, row in top_realistic.iterrows():\n",
        "    plt.text(row['AI_Median'] + 1000, row['STATE_NAME'], f\"+${int(row['Diff']):,}\", fontsize=8, va='center', color='black')\n",
        "\n",
        "plt.xlabel(\"Median Salary ($)\")\n",
        "plt.title(\"AI vs Traditional Salaries (Top 10 States, ≥300 AI Jobs)\")\n",
        "plt.legend()\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4ccd83e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "### Insight: Top States with Highest AI Salary Advantage (≥300 AI Jobs)\n",
        "\n",
        "This chart compares the median salaries of AI-related and traditional careers in states with substantial AI job markets (300+ postings). Each line connects the traditional (orange) and AI (blue) median salaries for a given state, with the annotated value indicating the salary gap.\n",
        "\n",
        "Key takeaways:\n",
        "- **Michigan, California, and Florida** exhibit the most substantial salary premiums for AI roles, with gaps exceeding **$30,000–$50,000**.\n",
        "- **New Jersey, Georgia, and Illinois** also show consistent advantages, reinforcing the value of AI specialization in large-scale markets.\n",
        "- By filtering for states with significant job volume, we ensure these salary gaps are **statistically meaningful**, not outliers from niche postings.\n",
        "\n",
        "This visualization reinforces that **AI careers aren't just higher paying — they're transformationally more lucrative** in regions where demand and infrastructure support sustainable opportunities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "81ef9dc1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Salaries Differ Across AI vs. Non-AI Careers\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: An LY\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The rapid evolution of Artificial Intelligence (AI) has significantly reshaped the job market—creating high-demand, specialized roles while also influencing the trajectory of traditional careers. This project explores how salaries differ between AI-related and non-AI occupations using real-world job postings from the Lightcast dataset. Through classification, aggregation, and visualization, we aim to uncover compensation trends across both career paths.\n",
        "\n",
        "## Dataset Overview\n",
        "\n",
        "We analyzed a total of 72,498 U.S. job postings from the Lightcast database. Each role was tagged as either an AI Career or a Non-AI Career using keyword matching on the TITLE_NAME and NAICS_2022_2_NAME columns.\n",
        "\n",
        "AI Careers: 28,310 postings(e.g., Data Scientist, Machine Learning Engineer, AI Engineer)\n",
        "\n",
        "Non-AI Careers: 44,188 postings(e.g., Retail Sales, Administrative Assistant, Customer Service)\n",
        "\n",
        "A custom rule-based function was used to assign each job into one of the two groups. We then filtered the dataset to include only postings with valid salary entries.\n"
      ],
      "id": "1832a199"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "df = pd.read_csv('lightcast_job_postings.csv', low_memory=False)\n",
        "\n",
        "job_counts = {'AI Careers': 28310, 'Non-AI Careers': 44188}\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(list(job_counts.keys()), list(job_counts.values()), color=['steelblue', 'gray'])\n",
        "plt.xlabel(\"Number of Job Postings\")\n",
        "plt.title(\"Distribution of AI vs. Non-AI Job Postings\")\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"DATA/job_distribution_bar_chart.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "676c906e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Salary Summary Table\n",
        "\n",
        "The salary summary table below provides a statistical overview of compensation across AI and non-AI careers. This analysis is derived strictly from the Lightcast job postings dataset after filtering for valid salary entries.\n",
        "\n",
        "Key metrics shown include mean, median, minimum, maximum, standard deviation, and the number of valid salary postings for each group. These statistics allow for a detailed comparison between the two career categories.\n"
      ],
      "id": "db63680e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "df = pd.read_csv('lightcast_job_postings.csv', low_memory=False)\n",
        "\n",
        "df['SALARY'] = pd.to_numeric(df['SALARY'], errors='coerce')\n",
        "\n",
        "def tag_ai(row):\n",
        "    title = str(row['TITLE_NAME']).lower()\n",
        "    industry = str(row['NAICS_2022_2_NAME']).lower()\n",
        "    \n",
        "    if ('ai' in title or \n",
        "        'artificial intelligence' in title or \n",
        "        'machine learning' in title or \n",
        "        'data scientist' in title or \n",
        "        'deep learning' in title or\n",
        "        'nlp' in title or\n",
        "        'computer vision' in title or\n",
        "        'robotics' in title or\n",
        "        'ml' in title or\n",
        "        'data engineer' in title or\n",
        "        'ml engineer' in title or\n",
        "        'scientist' in title or\n",
        "        ('professional, scientific, and technical services' in industry) or\n",
        "        ('information' in industry)):\n",
        "        return 'AI Career'\n",
        "    else:\n",
        "        return 'Non-AI Career'\n",
        "\n",
        "df['Career_Type'] = df.apply(tag_ai, axis=1)\n",
        "\n",
        "df_salary = df.dropna(subset=['SALARY'])\n",
        "\n",
        "mean_salary_df = df_salary.groupby('Career_Type')['SALARY'].mean().round(2).reset_index()\n",
        "mean_salary_df.columns = ['Career_Type', 'Mean_Salary ($)'] \n",
        "\n",
        "df['SALARY'] = pd.to_numeric(df['SALARY'], errors='coerce')\n",
        "df_salary = df.dropna(subset=['SALARY'])\n",
        "\n",
        "summary_stats = df_salary.groupby('Career_Type')['SALARY'].agg(\n",
        "    count='count',\n",
        "    mean='mean',\n",
        "    median='median',\n",
        "    min='min',\n",
        "    max='max',\n",
        "    std='std'\n",
        ").round(2)\n",
        "\n",
        "summary_stats = summary_stats.rename(columns={\n",
        "    'count': 'Valid Salary Entries',\n",
        "    'mean': 'Mean Salary ($)',\n",
        "    'median': 'Median Salary ($)',\n",
        "    'min': 'Minimum Salary ($)',\n",
        "    'max': 'Maximum Salary ($)',\n",
        "    'std': 'Standard Deviation ($)'\n",
        "})\n",
        "\n",
        "summary_stats['Job Postings Count'] = [28310 if i == 'AI Career' else 44188 for i in summary_stats.index]\n",
        "\n",
        "summary_stats = summary_stats[['Job Postings Count', 'Valid Salary Entries', 'Mean Salary ($)',\n",
        "                               'Median Salary ($)', 'Minimum Salary ($)', 'Maximum Salary ($)',\n",
        "                               'Standard Deviation ($)']]\n",
        "summary_stats"
      ],
      "id": "63ad11b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table clearly shows that AI-related careers offer higher mean and median salaries compared to non-AI careers. The minimum salary for AI jobs is also notably higher, indicating a stronger salary floor. Although both groups have similar standard deviations, suggesting comparable salary variability, the overall compensation level for AI roles is substantially greater.\n",
        "\n",
        "## Salary Distribution Visualization\n",
        "\n",
        "The following boxplot visualizes the salary distributions for AI versus non-AI career categories. It graphically represents the median, interquartile ranges, and outliers for each group.\n"
      ],
      "id": "844ca3b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('lightcast_job_postings.csv', low_memory=False)\n",
        "\n",
        "# Tag careers before subsetting\n",
        "def tag_ai(row):\n",
        "    title = str(row['TITLE_NAME']).lower()\n",
        "    industry = str(row['NAICS_2022_2_NAME']).lower()\n",
        "    if ('ai' in title or \n",
        "        'artificial intelligence' in title or \n",
        "        'machine learning' in title or \n",
        "        'data scientist' in title or \n",
        "        'deep learning' in title or \n",
        "        'nlp' in title or \n",
        "        'computer vision' in title or \n",
        "        'robotics' in title or \n",
        "        'ml' in title or \n",
        "        'data engineer' in title or \n",
        "        'ml engineer' in title or \n",
        "        'scientist' in title or \n",
        "        'professional, scientific, and technical services' in industry or \n",
        "        'information' in industry):\n",
        "        return 'AI Career'\n",
        "    else:\n",
        "        return 'Non-AI Career'\n",
        "\n",
        "df['Career_Type'] = df.apply(tag_ai, axis=1)\n",
        "\n",
        "# Now subset only rows with valid salary\n",
        "df_salary = df.dropna(subset=['SALARY'])\n",
        "\n",
        "# Plot without showing <Figure ...> output\n",
        "ax = df_salary.boxplot(column='SALARY', by='Career_Type', figsize=(10,6))\n",
        "plt.title('Salary Comparison: AI vs Non-AI Careers')\n",
        "plt.suptitle('')\n",
        "plt.xlabel('Career Type')\n",
        "plt.ylabel('Salary ($)')\n",
        "plt.grid(True)\n",
        "\n",
        "plot_path = os.path.join('DATA', 'ai_vs_nonai_salary_comparison.png')\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "id": "00eee8a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The boxplot highlights that AI careers have a higher median salary and a more compressed lower salary range compared to non-AI careers. While both groups exhibit high-end salary outliers (up to $500,000), non-AI careers show a greater spread of lower-end salaries. This suggests that AI careers offer more consistent and stable compensation, whereas non-AI jobs have more variability, particularly toward lower-paying positions.\n",
        "\n",
        "## Forecasting AI Career Trends \n",
        "To explore future patterns, we used a simple linear regression model to forecast average AI salaries and job posting volumes over the next six months.\n"
      ],
      "id": "3bc412d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "df = pd.read_csv('lightcast_job_postings.csv', low_memory=False)\n",
        "\n",
        "df['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\n",
        "df['SALARY'] = pd.to_numeric(df['SALARY'], errors='coerce')\n",
        "\n",
        "def tag_ai(row):\n",
        "    title = str(row['TITLE_NAME']).lower()\n",
        "    industry = str(row['NAICS_2022_2_NAME']).lower()\n",
        "    if ('ai' in title or 'artificial intelligence' in title or 'machine learning' in title or\n",
        "        'data scientist' in title or 'deep learning' in title or 'nlp' in title or\n",
        "        'computer vision' in title or 'robotics' in title or 'ml' in title or\n",
        "        'data engineer' in title or 'ml engineer' in title or 'scientist' in title or\n",
        "        'professional, scientific, and technical services' in industry or 'information' in industry):\n",
        "        return 'AI Career'\n",
        "    else:\n",
        "        return 'Non-AI Career'\n",
        "\n",
        "df['Career_Type'] = df.apply(tag_ai, axis=1)\n",
        "\n",
        "df_valid = df.dropna(subset=['POSTED'])\n",
        "monthly_counts = df_valid.groupby([df_valid['POSTED'].dt.to_period('M'), 'Career_Type']) \\\n",
        "                         .size().unstack().fillna(0)\n",
        "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "monthly_counts.plot(kind='line', marker='o', ax=plt.gca())\n",
        "plt.title(\"Monthly Job Postings: AI vs. Non-AI Careers\")\n",
        "plt.ylabel(\"Number of Postings\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "job_postings_path = \"DATA/monthly_job_postings_forecast.png\"\n",
        "plt.savefig(job_postings_path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "df_ai = df[(df['Career_Type'] == 'AI Career') & (~df['SALARY'].isna())]\n",
        "monthly_salary = df_ai.groupby(df_ai['POSTED'].dt.to_period('M'))['SALARY'].mean().dropna()\n",
        "monthly_salary.index = monthly_salary.index.to_timestamp()\n",
        "\n",
        "X = sm.add_constant(range(len(monthly_salary)))\n",
        "y = monthly_salary.values\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "future_periods = 6\n",
        "future_X = sm.add_constant(range(len(monthly_salary), len(monthly_salary) + future_periods))\n",
        "forecast = model.predict(future_X)\n",
        "\n",
        "last_date = monthly_salary.index[-1]\n",
        "future_dates = [last_date + pd.DateOffset(months=i+1) for i in range(future_periods)]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(monthly_salary.index, monthly_salary.values, marker='o', label='Historical Avg Salary (AI)')\n",
        "plt.plot(future_dates, forecast, marker='x', linestyle='--', label='Forecast (Next 6 Months)')\n",
        "plt.title(\"Forecast: Average AI Salary Over Time\")\n",
        "plt.ylabel(\"Salary ($)\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "salary_forecast_path = \"DATA/ai_salary_forecast.png\"\n",
        "plt.savefig(salary_forecast_path, dpi=300)\n",
        "plt.show()"
      ],
      "id": "35e3f64d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The analysis of monthly job postings reveals that Non-AI careers consistently dominate the U.S. job market, averaging around 9,000 postings per month. Despite experiencing a noticeable dip in July, Non-AI job volumes quickly rebounded in August and September, indicating strong demand recovery. In contrast, AI careers maintain a lower volume, typically ranging between 5,000 and 6,000 postings monthly. However, the trend among AI roles appears more stable, with minor fluctuations and a modest recovery following July’s decline. This suggests that while fewer in quantity, AI job postings demonstrate resilience and steady momentum in hiring activity.\n",
        "\n",
        "Looking at the salary trends for AI careers, historical data from May to September 2024 shows moderate fluctuations, with a pronounced dip in August representing the lowest average salary in that period. Using a simple linear regression model, forecasts from October 2024 through March 2025 indicate a gradual decline in average AI salaries. Importantly, this downward trend does not suggest collapse but rather points to a stabilizing market. As AI technologies become more widespread and accessible, the normalization of compensation—paired with a growing supply of skilled candidates—could be moderating previously inflated salary levels. Despite the projected dip, AI roles remain high-paying and continue to offer strong long-term potential.\n",
        "\n",
        "\n",
        "\n",
        "## Key Insights\n",
        "\n",
        "AI-related careers consistently offer higher salaries, with a mean of $133,344 and median of $130,500, compared to $108,513 and $102,500 respectively for non-AI roles.\n",
        "\n",
        "Salary variability remains relatively equal across both groups (~$43,000 standard deviation), but AI roles tend to avoid the lower-end salaries more common in non-AI positions.\n",
        "\n",
        "The minimum salary for AI careers is substantially higher at $23,585, indicating a stronger baseline earning potential versus $15,860 for non-AI roles.\n",
        "\n",
        "The maximum reported salary is identical across both categories ($500,000), likely reflecting high-level executive or specialized niche roles.\n",
        "\n",
        "AI job postings, while fewer in number, show consistent volume and a stable rebound after a mid-year dip, suggesting continued demand for AI talent.\n",
        "\n",
        "Forecasting results predict a gradual decline in AI average salaries over the next six months, indicating potential market stabilization rather than contraction.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This analysis confirms that AI-related roles provide superior salary outcomes, both in average and median terms, and offer greater compensation stability at the lower end of the spectrum. Despite a projected slight decline in average salaries, AI positions remain highly competitive and rewarding.\n",
        "\n",
        "These findings emphasize the value of developing AI-focused skill sets such as machine learning, NLP, and data engineering. For students, job seekers, and educators alike, aligning educational and career strategies with the AI sector’s continued evolution will be key to unlocking long-term financial and professional success.\n"
      ],
      "id": "0eddd082"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Exploratory Data Analysis\"\n",
        "subtitle: \"Uncovering Patterns in the Job Market Dataset\"\n",
        "author:\n",
        "  - name: Group 10\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This EDA explores key characteristics of the job postings dataset, including salary distribution, industry representation, geographic trends, and temporal patterns. The analysis provides insights that guide downstream tasks such as skill gap analysis, clustering, and prediction.\n",
        "\n",
        "# Dataset Overview\n"
      ],
      "id": "bb37b2ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "df.info()\n",
        "df.head()"
      ],
      "id": "3456cbc2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handling Missing Data\n"
      ],
      "id": "cb994886"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.isnull().mean().sort_values(ascending=False).head(10)"
      ],
      "id": "e8e9a0f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Salary Distribution\n"
      ],
      "id": "6fa69e97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df['SALARY'] = pd.to_numeric(df['SALARY'], errors='coerce')\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df['SALARY'], bins=50, kde=True)\n",
        "plt.title(\"Salary Distribution\")\n",
        "plt.xlabel(\"Salary ($)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xlim(0, 300000)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "429e5fc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top 10 Job Titles\n"
      ],
      "id": "9d936ebf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['TITLE_NAME'].value_counts().head(10)"
      ],
      "id": "f1f95f0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top 10 Industries\n"
      ],
      "id": "2f0e4d03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['NAICS_2022_2_NAME'].value_counts().head(10)"
      ],
      "id": "1b326273",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Job Postings by State\n"
      ],
      "id": "d8eafcbf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "state_counts = df['STATE_NAME'].value_counts().head(15)\n",
        "state_counts.plot(kind='barh', figsize=(10,6), color='steelblue')\n",
        "plt.title(\"Top 15 States by Job Postings\")\n",
        "plt.xlabel(\"Number of Postings\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e8a165aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Posting Trends Over Time\n"
      ],
      "id": "14741a57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['POSTED'] = pd.to_datetime(df['POSTED'], errors='coerce')\n",
        "df['POSTED'].dt.to_period('M').value_counts().sort_index().plot(kind='line', figsize=(12,6))\n",
        "plt.title(\"Job Postings Over Time\")\n",
        "plt.ylabel(\"Number of Postings\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.grid(True, alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "908b21b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df.columns.tolist())"
      ],
      "id": "e9283544",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "db7abb09"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Skill Gap Analysis\"\n",
        "subtitle: \"Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends\"\n",
        "author:\n",
        "  - name: \"Ritusri Mohan\"\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Skill Gap Analysis\n",
        "\n",
        "\n",
        "\n",
        "In this analysis, we evaluate the current skill levels of our team and compare them with industry requirements based on job postings data. This will help identify skill gaps and propose improvement strategies.\n",
        "\n",
        "\n",
        "\n",
        "## Team Skill Level Data\n"
      ],
      "id": "1c8877ec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: clean-data\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n",
        "    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n",
        "    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n",
        "]\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Drop columns with >50% missing\n",
        "df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
        "\n",
        "# Fill missing values\n",
        "for col in df.select_dtypes(include=[np.number]):\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "for col in df.select_dtypes(include=['object']):\n",
        "    df[col].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\", inplace=True)"
      ],
      "id": "clean-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top 5 and Top 8 Software Skills from Job Postings\n"
      ],
      "id": "22008bfc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: top-software-skills\n",
        "#| echo: true\n",
        "\n",
        "# Get top software skills from job postings\n",
        "software_skills = (\n",
        "    df['SOFTWARE_SKILLS_NAME']\n",
        "    .dropna()\n",
        "    .str.replace(r'[\\[\\]\\n]', '', regex=True)  # Remove brackets and newlines\n",
        "    .str.split(',')\n",
        "    .explode()\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# Filter out empty strings\n",
        "software_skills = software_skills[software_skills != '']\n",
        "\n",
        "# Get top 5 most frequent software skills\n",
        "top_5_software_skills = software_skills.value_counts().head(5)\n",
        "\n",
        "# Extract skill names\n",
        "top_5_skills = top_5_software_skills.index.tolist()\n",
        "\n",
        "print(\"Top 5 software skills from job postings:\")\n",
        "print(top_5_software_skills)\n",
        "\n",
        "# Get top 8 most frequent software skills\n",
        "top_8_software_skills = software_skills.value_counts().head(8)\n",
        "\n",
        "# Extract skill names\n",
        "top_8_skills = top_8_software_skills.index.tolist()\n",
        "\n",
        "print(\"Top 8 software skills from job postings:\")\n",
        "print(top_8_software_skills)"
      ],
      "id": "top-software-skills",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: team-skills-5\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define team skill levels (Proficiency scale: 1=Beginner, 5=Expert)\n",
        "skills_data = {\n",
        "    \"Name\": [\"Shreya\", \"An Ly\", \"Advait\", \"Ritusri\"],\n",
        "    \"SQL (Programming Language)\": [3, 2, 2, 3],\n",
        "    \"Microsoft Excel\": [4, 3, 3, 4],\n",
        "    \"Python (Programming Language)\": [3, 3, 3, 3],\n",
        "    \"SAP Applications\": [2, 1, 3, 2],\n",
        "    \"Dashboard\": [2, 2, 3, 4]\n",
        "}\n",
        "\n",
        "df_skills = pd.DataFrame(skills_data)\n",
        "df_skills.set_index(\"Name\", inplace=True)\n",
        "df_skills\n"
      ],
      "id": "team-skills-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: heatmap-top5\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Team skills for top 5\n",
        "skills_data_top5 = {\n",
        "    \"Name\": [\"Shreya\", \"An Ly\", \"Advait\", \"Ritusri\"],\n",
        "    \"SQL (Programming Language)\": [3, 2, 2, 3],\n",
        "    \"Microsoft Excel\": [4, 3, 3, 4],\n",
        "    \"Python (Programming Language)\": [3, 3, 3, 3],\n",
        "    \"SAP Applications\": [2, 1, 3, 2],\n",
        "    \"Dashboard\": [2, 2, 3, 4]\n",
        "}\n",
        "\n",
        "df_skills_top5 = pd.DataFrame(skills_data_top5).set_index(\"Name\")\n",
        "\n",
        "# Plot Heatmap 1\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df_skills_top5, annot=True, cmap=\"YlOrBr\", linewidths=0.5)\n",
        "plt.title(\"Team Skill Levels – Top 5 Software Skills\")\n",
        "plt.show()"
      ],
      "id": "heatmap-top5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: team-skills-8\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define team skill levels (Proficiency scale: 1=Beginner, 5=Expert)\n",
        "skills_data1 = {\n",
        "    \"Name\": [\"Shreya\", \"An Ly\", \"Advait\", \"Ritusri\"],\n",
        "    \"SQL (Programming Language)\": [3, 2, 2, 3],\n",
        "    \"Microsoft Excel\": [4, 3, 3, 4],\n",
        "    \"Python (Programming Language)\": [3, 3, 3, 3],\n",
        "    \"SAP Applications\": [2, 1, 3, 2],\n",
        "    \"Dashboard\": [2, 2, 3, 4],\n",
        "    \"Tableau (Business Intelligence Software)\":[0, 0, 0, 0],\n",
        "    \"Power BI\":[0, 0, 0, 0],\n",
        "    \"Microsoft Office\":[0, 0, 0, 0],\n",
        "    \n",
        "}\n",
        "\n",
        "df_skills1 = pd.DataFrame(skills_data1)\n",
        "df_skills1.set_index(\"Name\", inplace=True)\n",
        "df_skills1"
      ],
      "id": "team-skills-8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: heatmap-top8\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define team skill levels (Proficiency scale: 1=Beginner, 5=Expert)\n",
        "skills_data1 = {\n",
        "    \"Name\": [\"Shreya\", \"An Ly\", \"Advait\", \"Ritusri\"],\n",
        "    \"SQL (Programming Language)\": [3, 2, 2, 3],\n",
        "    \"Microsoft Excel\": [4, 3, 3, 4],\n",
        "    \"Python (Programming Language)\": [3, 3, 3, 3],\n",
        "    \"SAP Applications\": [2, 1, 3, 2],\n",
        "    \"Dashboard\": [2, 2, 3, 4],\n",
        "    \"Tableau (Business Intelligence Software)\":[0, 0, 0, 0],\n",
        "    \"Power BI\":[0, 0, 0, 0],\n",
        "    \"Microsoft Office\":[0, 0, 0, 0],\n",
        "    \n",
        "}\n",
        "df_skills_top8 = pd.DataFrame(skills_data1).set_index(\"Name\")\n",
        "\n",
        "# Plot Heatmap 2\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df_skills_top8, annot=True, cmap=\"YlOrBr\", linewidths=0.5)\n",
        "plt.title(\"Team Skill Levels – Top 8 Software Skills\")\n",
        "plt.show()"
      ],
      "id": "heatmap-top8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Improvement Plan\n",
        "The heatmap analysis indicates that while the team has solid grounding in fundamental IT tools, there are significant gaps in certain enterprise and visualization technologies that are highly demanded in job postings. Addressing these gaps will elevate the team’s capabilities and make them more market-ready.\n",
        "\n",
        "SQL (Programming Language)\n",
        "Although the team has intermediate proficiency in SQL, moving towards expert-level understanding is essential. SQL remains one of the most requested skills across IT and data-centric roles. Advanced topics like complex joins, window functions, performance tuning, and writing optimized queries should be covered. Recommended resources include Coursera's SQL for Data Science and DataCamp SQL Career Track.\n",
        "\n",
        "Python (Programming Language)\n",
        "Python is the cornerstone of automation, analytics, and machine learning. The team should focus on advancing their skills beyond basic scripting. This includes learning about data manipulation (Pandas), building small projects, and exploring libraries used in data science or automation. Free resources like the Google Python Crash Course and the IBM Python for Data Science (Coursera) can be leveraged.\n",
        "\n",
        "SAP Applications\n",
        "SAP is currently one of the weakest areas within the team. Enterprise software skills like SAP are critical in corporate environments. Team members should target introductory SAP courses to gain a basic understanding of navigation, reporting, and common SAP modules. Free options like OpenSAP are recommended to start with.\n",
        "\n",
        "Dashboarding Tools (Power BI / Tableau)\n",
        "Visual storytelling and data presentation are crucial skills in modern IT roles. As seen in the analysis, there is little to no current exposure to Power BI and Tableau. Training should start with beginner-friendly courses that teach how to create interactive dashboards and perform basic data analysis. Tableau Public and Microsoft Learn for Power BI offer excellent free resources.\n",
        "\n",
        "Microsoft Excel\n",
        "While the team is relatively advanced in Excel, moving towards expert proficiency will unlock greater efficiencies. Topics like VBA macros, Power Query, pivot tables, and advanced formulas should be prioritized. Recommended platforms include LinkedIn Learning and Excel Skills for Business (Coursera).\n",
        "\n",
        "Tableau, Power BI, and Microsoft Office (Missing Skills)\n",
        "The missing ratings for Tableau, Power BI, and Microsoft Office indicate complete lack of knowledge. These tools are essential for reporting, business communication, and project collaboration in IT environments. Each team member should aim to reach at least intermediate level proficiency through self-paced courses and hands-on practice.\n",
        "\n",
        "#### Suggested Team Learning Plan\n",
        "To close these gaps systematically:\n",
        "\n",
        "Weekly Learning Sessions: Rotate weekly responsibilities where each member shares what they learned about a specific skill.\n",
        "\n",
        "Pair Learning: Pair up team members with stronger skills (e.g., SQL, Python) to support others.\n",
        "\n",
        "Micro-Certifications: Encourage completion of short certifications within 4–6 weeks (e.g., Tableau Desktop Specialist, Power BI Fundamentals).\n",
        "\n",
        "Practical Projects: Apply new skills through internal mock projects like building dashboards or writing Python scripts.\n",
        "\n",
        "Tracking Progress: Regularly assess skill levels every month and update the heatmap to measure progress.\n",
        "\n",
        "### Conclusion\n",
        "The skill gap analysis clearly highlights the team’s current strengths and opportunities for improvement. While foundational skills like SQL, Python, and Excel are relatively well developed, enterprise tools like SAP, Tableau, and Power BI remain significant blind spots.\n",
        "\n",
        "By following the improvement plan and encouraging structured learning, the team will not only close existing gaps but also align with modern IT and data industry expectations. This proactive approach will make the team more versatile, industry-relevant, and ready for advanced career opportunities.\n",
        "\n",
        "---\n"
      ],
      "id": "6fb7cf51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"ML Methods\"\n",
        "subtitle: \"Predicting Job Posting Duration Using Random Forest Regressor\"\n",
        "author:\n",
        "  - name: \"Shreya Mani\"\n",
        "    affiliations:\n",
        "          - id: bu\n",
        "            name: Boston University\n",
        "            city: Boston\n",
        "            state: MA\n",
        "format: \n",
        "  html:\n",
        "        toc: true\n",
        "        number-sections: true\n",
        "        df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this machine learning project, I aimed to predict how long job postings remain active (i.e., their DURATION) using a Random Forest Regressor. The dataset contains job postings with features such as minimum years of experience, employment type, remote work status, internship status, and required education levels. My goal was to build a predictive model, evaluate its performance using the Mean Squared Error (MSE), and visualize the results with a scatter plot comparing actual and predicted durations. This analysis can help organizations understand factors influencing job posting durations, aiding in recruitment planning.\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "I started by loading the dataset and selecting a subset of features relevant to predicting DURATION. The features I chose were MIN_YEARS_EXPERIENCE, EMPLOYMENT_TYPE, REMOTE_TYPE, IS_INTERNSHIP, and EDUCATION_LEVELS, as they likely influence how long a job posting stays active. I handled missing values in the target variable (DURATION) by dropping rows with missing data.\n",
        "\n",
        "A challenge arose with the EDUCATION_LEVELS column, which contained string representations of lists . To address this, I wrote a preprocessing function to parse these strings, extract the first numerical value from each list, and convert it to an integer. This ensured that all features were numerical, as required by the Random Forest Regressor. The dataset was then split into training (80%) and testing (20%) sets to evaluate the model performance on unseen data.\n",
        "\n",
        "Here the Python code I used for data preprocessing:\n"
      ],
      "id": "d83e7503"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: ml data cleaning\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import ast\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Auto-download CSV if missing\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"{csv_path} not found! Attempting to download...\")\n",
        "    os.makedirs('region_analysis', exist_ok=True)\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        print(\"Installing gdown...\")\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # Replace with actual file ID\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    try:\n",
        "        gdown.download(url, csv_path, quiet=False)\n",
        "        print(\"Download complete!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        raise\n",
        "else:\n",
        "    print(f\"{csv_path} found. Proceeding...\")\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"Initial dataset size: {df.shape}\")\n",
        "    print(f\"Missing values:\\n{df[['DURATION', 'MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'MIN_EDULEVELS']].isnull().sum()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV: {e}\")\n",
        "    raise"
      ],
      "id": "ml-data-cleaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample of preprocessed EDUCATION_LEVELS\n"
      ],
      "id": "79dbd81a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define function to parse MIN_EDULEVELS strings\n",
        "def parse_education_levels(edu):\n",
        "    if isinstance(edu, (int, float)) and not np.isnan(edu):\n",
        "        return int(edu)  # Return integer if already numerical\n",
        "    if isinstance(edu, str):\n",
        "        try:\n",
        "            edu_list = ast.literal_eval(edu.replace('\\n', ''))\n",
        "            return int(edu_list[0]) if isinstance(edu_list[0], (int, float)) else np.nan\n",
        "        except (ValueError, SyntaxError, IndexError) as e:\n",
        "            print(f\"Parsing failed for: {edu}, Error: {e}\")\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "# Select features and target\n",
        "features = ['MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'IS_INTERNSHIP', 'MIN_EDULEVELS']\n",
        "target = 'DURATION'\n",
        "\n",
        "# Check if all features and target exist\n",
        "missing_cols = [col for col in features + [target] if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Missing columns: {missing_cols}\")\n",
        "    # If IS_INTERNSHIP is missing, remove it from features\n",
        "    if 'IS_INTERNSHIP' in missing_cols:\n",
        "        features.remove('IS_INTERNSHIP')\n",
        "    else:\n",
        "        raise ValueError(\"Required columns not found in dataset\")\n",
        "\n",
        "# Create a copy of the dataset with selected columns\n",
        "df_subset = df[features + [target]].copy()\n",
        "\n",
        "# Parse MIN_EDULEVELS\n",
        "df_subset['MIN_EDULEVELS'] = df_subset['MIN_EDULEVELS'].apply(parse_education_levels)\n",
        "\n",
        "# Handle missing values with imputation for all numerical columns\n",
        "num_cols = [col for col in features if col in df_subset.columns]\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "df_subset[num_cols] = num_imputer.fit_transform(df_subset[num_cols])\n",
        "\n",
        "# Impute DURATION with mean\n",
        "df_subset['DURATION'] = df_subset['DURATION'].fillna(df_subset['DURATION'].mean())\n",
        "\n",
        "# Ensure IS_INTERNSHIP is integer if present\n",
        "if 'IS_INTERNSHIP' in df_subset.columns:\n",
        "    df_subset['IS_INTERNSHIP'] = df_subset['IS_INTERNSHIP'].astype(int)\n",
        "\n",
        "# Verify no missing values\n",
        "print(f\"Missing values after imputation:\\n{df_subset.isnull().sum()}\")\n",
        "print(f\"Preprocessed dataset size: {df_subset.shape}\")\n",
        "\n",
        "# Features and target\n",
        "X = df_subset[num_cols]\n",
        "y = df_subset['DURATION']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)\n",
        "print(\"Sample of preprocessed MIN_EDULEVELS:\", df_subset['MIN_EDULEVELS'].head().tolist())"
      ],
      "id": "9de29727",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training\n",
        "\n",
        "With the data preprocessed, I trained a Random Forest Regressor, a robust model that handles numerical features well and is less prone to overfitting. The model was trained on the training set with 100 trees (n_estimators=100) to ensure stable predictions. Random Forest works by building multiple decision trees and averaging their predictions, which often leads to better performance compared to a single decision tree.\n",
        "\n",
        "Here is the code for training the Random Forest Regressor:\n"
      ],
      "id": "9443d965"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")"
      ],
      "id": "ac38fb82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Visualization\n",
        "After training the model, I used it to predict the DURATION for the test set. To evaluate the model performance, I calculated the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values. A lower MSE indicates better predictive accuracy.\n",
        "\n",
        "I also created a scatter plot to visualize the model performance, comparing the actual DURATION values to the predicted ones. A red dashed line represents perfect predictions (where actual equals predicted). Points closer to this line indicate better predictions.\n",
        "\n",
        "Here is the code for evaluation and visualization:\n"
      ],
      "id": "8c5c7290"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_mse = -cv_scores.mean()\n",
        "cv_rmse = np.sqrt(cv_mse)\n",
        "print(f\"Cross-validated MSE: {cv_mse:.2f} ± {cv_scores.std():.2f}\")\n",
        "print(f\"Cross-validated RMSE: {cv_rmse:.2f} days\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate MSE and RMSE on test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Test set MSE: {mse:.2f}\")\n",
        "print(f\"Test set RMSE: {rmse:.2f} days\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': num_cols,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Predicted vs Actual')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel('Actual Duration (Days)')\n",
        "plt.ylabel('Predicted Duration (Days)')\n",
        "plt.title('Random Forest Regressor: Actual vs Predicted Job Posting Duration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('actual_vs_predicted_duration.png')\n",
        "plt.show()"
      ],
      "id": "700561f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "The Mean Squared Error (MSE) provides a quantitative measure of the model performance. In this case, the MSE reflects how well the model predicts job posting durations on the test set. The scatter plot (actual_vs_predicted_duration.png) visually demonstrates the model accuracy. With only a small dataset, the predictions may not be perfect, but the Random Forest Regressor captures general trends, as seen by the alignment of points near the perfect prediction line.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Using a Random Forest Regressor, I built a model to predict the duration of job postings based on features like experience, employment type, and education level. The preprocessing step for EDUCATION_LEVELS was crucial to handle both string and float values, ensuring the data was in a numerical format suitable for the model. However, the model performance was poor, with an MSE of 1296.00 and a significant overprediction (42.0 days predicted vs. 6.0 days actual), as shown in the scatter plot. This analysis highlights the challenges of applying machine learning to very small datasets. Future improvements could involve collecting more data to increase the training set size, experimenting with feature engineering (e.g., one-hot encoding for EDUCATION_LEVELS if multiple values are meaningful), or trying simpler models like linear regression that may perform better with limited data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "7c749dbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"NLP Methods\"\n",
        "subtitle: \"NLP Analysis: Extracting Required Skills from Job Postings\"\n",
        "author:\n",
        "  - name: Shreya Mani\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "        toc: true\n",
        "        number-sections: true\n",
        "        df-print: paged\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Introduction\n",
        "In this project, we used Natural Language Processing (NLP) to extract required skills from job postings based on their description text in the BODY column. The dataset contains job postings with unstructured text descriptions, which often mention skills needed for the role (e.g., \"analyze data\" or \"develop software\"). Our goal was to identify and analyze the most common skills mentioned in these postings, providing insights into the skills most in demand. This analysis can help job seekers understand the key skills to develop and assist employers in identifying trends in skill requirements.\n",
        "\n",
        "# Data Preprocessing\n",
        "We started by loading the dataset and focusing on the BODY column, which contains the job description text. The BODY text is unstructured and requires preprocessing for NLP analysis. We performed the following steps:\n",
        "\n",
        "Tokenization and Cleaning: Converted the text to lowercase, removed punctuation, and tokenized the text into words.\n",
        "\n",
        "\n",
        "Stop Word Removal: Removed common stop words (e.g., \"the\", \"is\") that don’t add meaningful information. We used a predefined list of common stop words to avoid external dependencies.\n",
        "\n",
        "\n",
        "Skill Extraction: Defined a list of common skills relevant to job postings (e.g., \"data analysis,\" \"software development\") and searched for these skills in the cleaned text. For simplicity, we used keyword matching to identify skills, but this could be extended with more advanced NLP techniques like named entity recognition (NER) or pre-trained models.\n",
        "\n",
        "Since this task is exploratory and doesn’t require a target variable, we didn’t split the data into training and testing sets. Instead, we processed all available job descriptions to extract and analyze skills.\n",
        "\n",
        "Here’s the Python code we used for data preprocessing and skill extraction:\n"
      ],
      "id": "7a22297c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: skill-extraction\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Drop rows with missing job descriptions\n",
        "df = df.dropna(subset=['BODY'])\n",
        "\n",
        "# Define stop words\n",
        "stop_words = {\n",
        "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
        "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will',\n",
        "    'with', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
        "    'yours', 'yourself', 'yourselves', 'him', 'his', 'her', 'hers', 'herself', 'it',\n",
        "    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
        "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
        "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
        "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
        "    'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
        "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
        "    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
        "}\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    return ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "df['BODY_CLEANED'] = df['BODY'].apply(clean_text)\n",
        "\n",
        "# Define common skills\n",
        "skills_list = [\n",
        "    \"data analysis\", \"software development\", \"machine learning\",\n",
        "    \"project management\", \"communication\", \"teamwork\",\n",
        "    \"sql\", \"python\", \"modeling\", \"analytics\"\n",
        "]\n",
        "\n",
        "# Extract skills\n",
        "def extract_skills(text):\n",
        "    found_skills = []\n",
        "    for skill in skills_list:\n",
        "        if skill in text:\n",
        "            found_skills.append(skill)\n",
        "    return found_skills\n",
        "\n",
        "df['SKILLS'] = df['BODY_CLEANED'].apply(extract_skills)\n",
        "\n",
        "# Flatten skill list and count\n",
        "all_skills = [skill for sublist in df['SKILLS'] for skill in sublist]\n",
        "skill_counts = Counter(all_skills)\n",
        "\n",
        "\n",
        "\n",
        "# Visualize top skills\n",
        "plt.figure(figsize=(10, 6))\n",
        "if skill_counts:\n",
        "    skills, counts = zip(*sorted(skill_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "    plt.bar(skills, counts, color='skyblue')\n",
        "    plt.xlabel('Skills')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Most Common Skills in Job Postings')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('skills_frequency.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No skills were found in the job descriptions.\")"
      ],
      "id": "skill-extraction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skill Analysis and Visualization\n",
        "\n",
        "After extracting skills from the job descriptions, we analyzed their frequency to identify the most common skills mentioned. We visualized the results using a bar plot, showing the count of each skill across all job postings. This helps highlight the skills that are most in demand based on the dataset.\n",
        "\n",
        "Here’s the code we used for analyzing and visualizing the skills:\n"
      ],
      "id": "013626f6"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Career Strategy\"\n",
        "subtitle: \"Personal Career Roadmap Based on 2024 Job Market Trends\"\n",
        "author:\n",
        "  - name: \"An Ly, Advait Pillai, Ritusri Mohan, Shreya Mani \"\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "bibliography: references.bib\n",
        "csl: csl/econometrica.csl\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# 1. An Ly – Business Analyst in Finance or Tech\n",
        "\n",
        "## Career Goal\n",
        "\n",
        "My goal is to secure a Business Analyst role in either the financial services or technology industry. I'm particularly interested in companies that use analytics to drive strategic decision-making, especially in hybrid or remote-friendly environments.\n",
        "\n",
        "## Key Trends Supporting My Career Plan\n",
        "\n",
        "Our EDA revealed that roles in professional, scientific, and technical services dominate job postings in 2024. Remote roles are especially prevalent in tech, aligning well with my flexibility needs and skills in Python, Tableau, and SQL.\n",
        "\n",
        "## Action Plan\n",
        "\n",
        "1. Strengthen Python and Power BI visualization skills.\n",
        "2. Network with Boston alumni working in fintech.\n",
        "3. Apply to hybrid analyst roles in NY, MA, and CA.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Advait Pillai – Data Engineer in Cloud & Big Data\n",
        "\n",
        "## Career Goal\n",
        "\n",
        "I aim to become a data engineer focused on big data pipelines and cloud infrastructure, particularly with AWS and Spark.\n",
        "\n",
        "## Key Trends Supporting My Career Plan\n",
        "\n",
        "The job title analysis showed strong demand for technical roles like Data Engineer, especially in major tech hubs like California and Texas. Our project also highlighted the increasing importance of cloud-based roles with remote options.\n",
        "\n",
        "## Action Plan\n",
        "\n",
        "1. Earn AWS Cloud Practitioner and Spark certifications.\n",
        "2. Complete hands-on projects in ETL and data pipeline development.\n",
        "3. Target roles at cloud-first companies like Snowflake, Amazon, and startups.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Ritusri Mohan – Business Intelligence Analyst in Healthcare\n",
        "\n",
        "## Career Goal\n",
        "\n",
        "I want to pursue a career as a Business Intelligence Analyst within the healthcare industry. I’m passionate about combining analytics and patient-centric systems to improve operations.\n",
        "\n",
        "## Key Trends Supporting My Career Plan\n",
        "\n",
        "Healthcare & Social Assistance was the second-largest industry by job postings in our dataset. However, most roles remain on-site, which aligns with my comfort level working in physical healthcare environments.\n",
        "\n",
        "## Action Plan\n",
        "\n",
        "1. Complete Tableau + SQL + Healthcare Analytics specialization on Coursera.\n",
        "2. Attend HIMSS career webinars and healthcare data meetups.\n",
        "3. Apply for entry-level BI roles in hospitals or health tech firms.\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Shreya Mani – Data Scientist in Retail or Consulting\n",
        "\n",
        "## Career Goal\n",
        "\n",
        "My goal is to become a Data Scientist specializing in customer behavior, forecasting, or supply chain analytics in the retail or consulting sectors.\n",
        "\n",
        "## Key Trends Supporting My Career Plan\n",
        "\n",
        "Our analysis of job titles and word clouds indicated demand for data and consulting roles. While retail showed fewer postings, consulting and analytics-driven firms remain strong in hiring for hybrid data roles.\n",
        "\n",
        "## Action Plan\n",
        "\n",
        "1. Build end-to-end forecasting models using time-series libraries.\n",
        "2. Apply to rotational data science programs at firms like Deloitte and Target.\n",
        "3. Practice Kaggle competitions and publish findings to GitHub/LinkedIn.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Each of us is aligning our personal goals with the trends we uncovered through our analysis of 2024 job market data. Whether targeting technical infrastructure, healthcare analytics, or strategic consulting, we aim to use our data-driven insights to inform smart, personalized job search strategies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "1fd26817"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"References\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Predictive Salary Modelling: Leveraging Data Science Skills and Machine Learning for Accurate Forecasting\n",
        "Haseeb, M. A., Viswanathan, R., Iyer, K., Hota, A. R., & Prathaban, B. P. (2024). Predictive salary modelling: Leveraging data science skills and machine learning for accurate forecasting. 2024 9th International Conference on Communication and Electronics Systems (ICCES), 1011–1019. https://ieeexplore.ieee.org/document/10859447\n",
        "\n",
        "## Tackling Economic Inequalities Through Business Analytics: A Literature Review\n",
        "Adaga, E. M., Egieya, Z. E., Ewuga, S. K., Abdul, A. A., & Abrahams, O. (2024). Tackling economic inequalities through business analytics: A literature review. Computer Science & IT Research Journal, 5(1), 60–80. https://doi.org/10.51594/csitjr.v5i1.702\n",
        "\n",
        "## Antecedent Configurations Toward Supply Chain Resilience: The Joint Impact of Supply Chain Integration and Big Data Analytics Capability\n",
        "Jiang, Y., Feng, T., & Huang, Y. (2024). Antecedent configurations toward supply chain resilience: The joint impact of supply chain integration and big data analytics capability. Journal of Operations Management, 70(2), 257–284. https://doi.org/10.1002/joom.1282\n",
        "\n",
        "## Leveraging AI and Data Analytics for Enhancing Financial Inclusion in Developing Economies\n",
        "Adeoye, O. B., Addy, W. A., Ajayi-Nifise, A. O., Odeyemi, O., Okoye, C. C., & Ofodile, O. C. (n.d.). Leveraging AI and data analytics for enhancing financial inclusion in developing economies. Finance & Accounting Research Journal. https://fepbl.com/index.php/farj/article/view/856\n",
        "\n",
        "## Employee Career Decision Making: The Influence of Salary and Benefits, Work Environment and Job Security\n",
        "Achim, N., Badrolhisam, N. I., & Zulkipli, N. (2019). Employee career decision making: The influence of salary and benefits, work environment and job security. Journal of Academia, 7(Special Issue 1), 41–50.\n",
        "\n",
        "## The Influence of Salaries and “Opportunity Costs” on Teachers’ Career Choices\n",
        "Murnane, R. J., Singer, J. D., & Willett, J. B. (1989). The influences of salaries and “opportunity costs” on teachers' career choices: Evidence from North Carolina. Harvard Educational Review, 59(3), 325–349.\n",
        "\n",
        "## The Future of Work: Impacts of AI on Employment and Job Market Dynamics\n",
        "Tomar, A., Sharma, S., Arti, & Suman, S. (2024). The future of work: Impacts of AI on employment and job market dynamics. 2024 International Conference on Progressive Innovations in Intelligent Systems and Data Science (ICPIDS).\n",
        "\n",
        "## AI and Job Market: Analysing the Potential Impact of AI on Employment, Skills, and Job Displacement\n",
        "Faluyi, S. E. (2025). AI and job market: Analysing the potential impact of AI on employment, skills, and job displacement. African Journal of Marketing Management, 17(1), 1–8.\n"
      ],
      "id": "8f2d286b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Library/Frameworks/Python.framework/Versions/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}