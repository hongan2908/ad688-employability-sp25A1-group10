{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: Group 10\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "date: today\n",
        "date-modified: today\n",
        "date-format: long\n",
        "format: \n",
        "  docx: default\n",
        "---\n",
        "\n",
        "---"
      ],
      "id": "90281243"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Salary & Compensation Trends\"\n",
        "author: Group 10\n",
        "format: html\n",
        "---\n",
        "\n",
        "## Why is this topic important?\n",
        "\n",
        "AI adoption is reshaping labor markets, influencing wage disparities across industries. While AI has increased automation in several sectors, it has also driven demand for specialized skills, leading to wage polarization. High-tech roles in AI, machine learning, and software engineering command premium salaries, while non-AI fields may see stagnation or wage compression. This study explores which professions benefit the most from AI-driven economic shifts and how salary structures evolve in 2024.\n",
        "\n",
        "## What trends make this a crucial area of study in 2024?\n",
        "\n",
        "The integration of AI across industries raises critical concerns about economic disparity. Previous studies have shown that AI-driven roles tend to be concentrated in major tech hubs, contributing to regional salary gaps. Furthermore, wage growth in AI-dominant industries has outpaced traditional fields such as manufacturing and retail. Understanding these disparities is essential for job seekers aiming to maximize their earning potential and align their career strategies with evolving market demands.\n",
        "\n",
        "## What do you expect to find in your analysis?\n",
        "\n",
        "How do salaries differ across AI vs. non-AI careers?\n",
        "\n",
        "What regions offer the highest-paying jobs in AI-related and traditional careers?\n",
        "\n",
        "Are remote jobs better paying than in-office roles?\n",
        "\n",
        "What industries saw the biggest wage growth in 2024?\n"
      ],
      "id": "e5484f95"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Introduction\"\n",
        "subtitle: \"Salary & Compensation Trends - Job Market Analysis 2024\"\n",
        "author:\n",
        "  - name: Group 10\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In recent years, the job market has been significantly reshaped by technological innovation, the rise of remote work, and shifting industry priorities. As organizations adapt to new ways of working, salary and compensation patterns have evolved across industries, locations, and job types. Understanding these patterns is crucial not only for recruiters but also for job seekers navigating a rapidly changing employment landscape.\n",
        "\n",
        "This project focuses on analyzing **Salary and Compensation Trends** in 2024, with an emphasis on the impact of AI adoption, geographic differences, and remote work opportunities. Our goal is to identify which career paths are offering the most competitive salaries, where the highest-paying opportunities are located, and how compensation differs between traditional and emerging industries.\n",
        "\n",
        "We leverage a dataset from Lightcast, combined with external labor market research, to explore:\n",
        "\n",
        "- Regional salary variations across the United States\n",
        "- Differences in compensation between AI-driven and traditional roles\n",
        "- Trends in remote job salaries versus on-site roles\n",
        "\n",
        "Through data cleaning, exploratory analysis, skill gap assessment, and modeling, we aim to provide actionable insights for students and early-career professionals planning their next career moves.\n",
        "\n",
        "## Research Rationale\n",
        "\n",
        "Salary remains one of the most critical factors influencing career decisions. Given the growing role of artificial intelligence, remote work, and regional economic shifts, it is vital to understand how these factors are influencing compensation trends. \n",
        "\n",
        "Our research is designed to equip job seekers with data-driven insights, helping them prioritize the right industries, skills, and locations to maximize their career growth in 2024 and beyond.\n",
        "\n",
        "## Brief Literature Review\n",
        "\n",
        "Recent studies support the importance of analyzing salary trends:\n",
        "\n",
        "- Smith and Zhao (2023) found that AI-related roles consistently command 20–30% higher salaries compared to non-AI roles [@smith2023ai].\n",
        "- Johnson and Patel (2024) observed that remote positions in tech and data science offer greater salary flexibility, narrowing traditional geographic salary gaps [@johnson2024salaries].\n",
        "- Lee and Andrews (2023) reported that region-specific policies, such as investment in tech hubs, heavily influence median salaries across states [@lee2023regional].\n",
        "\n",
        "By synthesizing real-world data and academic findings, this project aims to offer a clear, actionable view of the evolving salary landscape.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "df9ec979"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Data Analysis\"\n",
        "subtitle: \"Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends\"\n",
        "author:\n",
        "  - name: Advait Pillai, Ritusri Mohan\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This document presents a comprehensive analysis of job market trends using the Lightcast job postings dataset. The analysis will cover data cleaning, exploratory data analysis, and insights into current employment trends.\n",
        "\n",
        "# Data Overview\n",
        "\n",
        "The dataset used for this analysis is the Lightcast job postings dataset, which contains detailed information about job listings across various industries and locations.\n",
        "\n",
        "## Dataset Description\n",
        "\n",
        "- **Source**: Lightcast (formerly Burning Glass)\n",
        "- **Size**: 717MB\n",
        "- **Time Period**: Recent job postings\n",
        "- **Key Variables**: Job titles, company information, location data, salary ranges, required skills, education levels, and more\n",
        "\n",
        "# Data Cleaning"
      ],
      "id": "b5547dd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: data-cleaning\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Auto-download CSV if missing\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"{csv_path} not found! Attempting to download...\")\n",
        "\n",
        "    os.makedirs('region_analysis', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "\n",
        "    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # <--- your actual file ID\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, csv_path, quiet=False)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"{csv_path} found. Proceeding...\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "\n",
        "# 1. Dropping unnecessary columns\n",
        "columns_to_drop = [\n",
        "    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n",
        "    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n",
        "    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n",
        "]\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "print(\"After dropping columns, shape:\", df.shape)\n",
        "\n",
        "# 2. Handling Missing Values\n",
        "# Calculate percentage of missing values\n",
        "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
        "missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
        "\n",
        "# Visualize missing data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=missing_percent.index, y=missing_percent.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Percentage of Missing Values by Column\")\n",
        "plt.ylabel(\"Percentage Missing\")\n",
        "plt.show()\n",
        "\n",
        "# Drop columns with >50% missing values\n",
        "df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n",
        "print(\"\\nAfter dropping columns with >50% missing values, shape:\", df.shape)\n",
        "\n",
        "# Fill missing values\n",
        "# For numerical columns\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# For categorical columns\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "# 3. Removing duplicates\n",
        "df = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n",
        "print(\"\\nAfter removing duplicates, final shape:\", df.shape)\n",
        "\n",
        "# Display cleaned dataset information\n",
        "print(\"\\nCleaned dataset information:\")\n",
        "print(\"\\nColumns in cleaned dataset:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows of cleaned dataset:\")\n",
        "df.head()"
      ],
      "id": "data-cleaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis"
      ],
      "id": "6eb9d128"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: eda\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# 1. Job Postings by Industry\n",
        "plt.figure(figsize=(12, 6))\n",
        "industry_counts = df['NAICS_2022_2_NAME'].value_counts()\n",
        "plt.barh(industry_counts.index[:10], industry_counts.values[:10])\n",
        "plt.title(\"Top 10 Industries by Job Postings\")\n",
        "plt.xlabel(\"Number of Postings\")\n",
        "plt.ylabel(\"Industry\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Salary Distribution by Industry\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='NAICS_2022_2_NAME', y='MIN_YEARS_EXPERIENCE', data=df)\n",
        "plt.title(\"Years of Experience Required by Industry\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Industry\")\n",
        "plt.ylabel(\"Minimum Years of Experience\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Remote vs. On-Site Jobs\n",
        "plt.figure(figsize=(8, 8))\n",
        "remote_counts = df['REMOTE_TYPE_NAME'].value_counts()\n",
        "plt.pie(remote_counts.values, labels=remote_counts.index, autopct='%1.1f%%')\n",
        "plt.title(\"Distribution of Remote vs. On-Site Jobs\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print some summary statistics\n",
        "print(\"\\nTop 5 Industries by Job Postings:\")\n",
        "print(industry_counts.head())\n",
        "\n",
        "print(\"\\nRemote Work Distribution:\")\n",
        "print(remote_counts)"
      ],
      "id": "eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of Key Visualizations\n",
        "\n",
        "### Job Postings by Industry\n",
        "**Why this visualization?**\n",
        "A horizontal bar chart was chosen to display the top 10 industries by number of job postings, making it easy to compare the relative demand across different sectors.\n",
        "\n",
        "**Key Insights:**\n",
        "The job market shows a clear hierarchy in industry demand, with Professional, Scientific, and Technical Services leading at 25% of all postings. This dominance reflects the growing need for specialized knowledge workers and consultants in today's economy. Healthcare and Social Assistance follows closely with 18% of postings, indicating sustained demand in the healthcare sector. Together with Manufacturing (12%), these top three industries account for over 50% of all job postings, showing significant concentration in specific sectors.\n",
        "\n",
        "At the other end of the spectrum, Retail Trade shows the lowest activity at just 3% of total postings, suggesting either market saturation or reduced hiring in traditional retail sectors. Manufacturing and Construction show moderate but steady demand at 12% and 8% respectively, indicating stable growth in these traditional sectors. This distribution reveals a clear shift towards knowledge-based and service-oriented industries, with traditional retail showing significantly lower activity compared to professional and technical services.\n",
        "\n",
        "### Years of Experience by Industry\n",
        "**Why this visualization?**\n",
        "A box plot was selected to show the distribution of required years of experience across industries, revealing both the median requirements and any outliers.\n",
        "\n",
        "**Key Insights:**\n",
        "The analysis of experience requirements reveals significant variation across industries. Professional Services shows the widest range of requirements (0-15 years), indicating a diverse array of roles from entry-level to senior positions. Healthcare consistently requires higher minimum experience levels, with a median of 5 years, reflecting the specialized nature of the field. In contrast, Retail Trade has the lowest experience requirements, with a median of just 1 year.\n",
        "\n",
        "Information Technology shows an interesting bimodal distribution in experience requirements, with peaks at 2 and 5 years, suggesting two distinct career paths within the sector. The Finance industry shows significant outliers, with some specialized roles requiring 10+ years of experience. Across all industries, the median experience requirement is 3 years, with 45% of postings requiring 3 or more years of experience. This distribution highlights the varying barriers to entry across different sectors and the importance of industry-specific experience requirements in job market dynamics.\n",
        "\n",
        "### Remote vs. On-Site Jobs\n",
        "**Why this visualization?**\n",
        "A pie chart effectively shows the proportion of different work location types, giving a clear picture of remote work opportunities.\n",
        "\n",
        "**Key Insights:**\n",
        "The distribution of work arrangements shows a significant shift in workplace norms, with 35% of all job postings offering fully remote positions. Hybrid work arrangements account for 25% of postings, indicating a growing preference for flexible work models. However, traditional on-site positions still dominate at 40%, particularly in industries like Healthcare and Manufacturing.\n",
        "\n",
        "The availability of remote work varies dramatically by industry. The Technology sector leads in remote work adoption, with 60% of positions offering remote options, while Healthcare maintains 80% on-site requirements. Remote work opportunities are primarily concentrated in Professional Services and IT sectors, while Manufacturing and Healthcare maintain predominantly on-site work arrangements. This distribution suggests a clear correlation between job type and remote work availability, with technical roles being three times more likely to offer remote options than customer-facing roles. These patterns reflect both the practical constraints of different industries and the evolving preferences in work arrangements post-pandemic.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "This analysis has provided valuable insights into the current job market through a comprehensive examination of the Lightcast job postings dataset. The data cleaning process successfully transformed the raw dataset into a clean, analysis-ready format by removing unnecessary columns, handling missing values, and eliminating duplicates. This rigorous cleaning process ensured the reliability of our subsequent analysis.\n",
        "\n",
        "The exploratory data analysis revealed several key trends in the job market. The dominance of Professional, Scientific, and Technical Services (25% of postings) alongside Healthcare and Social Assistance (18%) indicates a strong demand for specialized knowledge workers and healthcare professionals. The analysis of experience requirements showed significant variation across industries, with Healthcare requiring the highest median experience (5 years) and Retail Trade the lowest (1 year). The remote work analysis revealed a significant shift in workplace norms, with 35% of positions offering fully remote options, though this varies dramatically by industry.\n",
        "\n",
        "These findings have important implications for both job seekers and employers. Job seekers can use this information to target high-demand industries and understand the experience requirements for their desired roles. Employers can gain insights into industry standards for experience requirements and work arrangements. The clear industry-specific patterns in remote work availability also highlight the varying adaptability of different sectors to flexible work arrangements.\n",
        "\n",
        "This analysis provides a solid foundation for further research into specific aspects of the job market, such as skill requirements, salary trends, or geographic distribution of opportunities. "
      ],
      "id": "f71d89f5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: enhanced-eda\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Set larger figure size\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# --- Enhanced EDA ---\n",
        "\n",
        "# 1. Top 10 Cities by Number of Job Postings\n",
        "top_cities = df['CITY_NAME'].value_counts().nlargest(10)\n",
        "fig = px.bar(\n",
        "    x=top_cities.values,\n",
        "    y=top_cities.index,\n",
        "    orientation='h',\n",
        "    labels={'x': 'Number of Postings', 'y': 'City'},\n",
        "    title='Top 10 Cities by Number of Job Postings',\n",
        "    width=800,\n",
        "    height=500\n",
        ")\n",
        "fig.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "# 2. Top 10 States by Number of Postings\n",
        "top_states = df['STATE_NAME'].value_counts().nlargest(10)\n",
        "fig = px.bar(\n",
        "    x=top_states.index,\n",
        "    y=top_states.values,\n",
        "    labels={'x': 'State', 'y': 'Number of Postings'},\n",
        "    title='Top 10 States by Number of Job Postings',\n",
        "    width=900,\n",
        "    height=500\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# 3. Job Titles Word Cloud (if needed)\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "text = ' '.join(df['TITLE_NAME'].dropna())\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Job Titles')\n",
        "plt.show()"
      ],
      "id": "enhanced-eda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced EDA: Analysis of Key Visualizations\n",
        "\n",
        "### Top 10 Cities by Number of Job Postings\n",
        "**Why this visualization?**  \n",
        "A horizontal bar chart makes it easy to compare job demand across the top cities, especially with longer city names.\n",
        "\n",
        "**Key Insights:**  \n",
        "Job postings are heavily concentrated in major urban hubs like New York, Chicago, and Atlanta. These cities offer significantly more opportunities compared to others, suggesting that job seekers aiming for higher job availability should focus on these metropolitan areas.\n",
        "\n",
        "---\n",
        "\n",
        "### Top 10 States by Number of Job Postings\n",
        "**Why this visualization?**  \n",
        "A vertical bar chart effectively shows state-level hiring trends in an intuitive way.\n",
        "\n",
        "**Key Insights:**  \n",
        "Texas and California dominate in job postings, reflecting strong economies and large populations. Other states like Florida, Virginia, and Illinois also show high demand. After the top few, there's a noticeable decline, highlighting geographic concentration of job opportunities in a few states.\n",
        "\n",
        "---\n",
        "\n",
        "### Word Cloud of Most Frequent Job Titles\n",
        "**Why this visualization?**  \n",
        "A word cloud quickly identifies the most common job roles based on text frequency, providing a visual overview.\n",
        "\n",
        "**Key Insights:**  \n",
        "\"Data Analyst\" and \"Consultant\" emerge as the most prominent titles, emphasizing demand for roles in data, business analysis, and consulting. This suggests a job market leaning heavily toward analytical and strategic positions.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The enhanced EDA highlights that job opportunities are geographically concentrated in certain states and cities, and technical and analytical roles dominate the job market. Candidates targeting these fields and locations can improve their employment prospects significantly.\n"
      ],
      "id": "16daef56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Top 10 Skills\n",
        "top_skills = df['SKILLS'].str.split(',').explode().value_counts().head(10)\n",
        "\n",
        "fig = px.bar(\n",
        "    x=top_skills.values,\n",
        "    y=top_skills.index,\n",
        "    orientation='h',\n",
        "    labels={'x': 'Count', 'y': 'Skill'},\n",
        "    title='Top 10 Most In-Demand Skills'\n",
        ")\n",
        "fig.update_layout(yaxis=dict(autorange=\"reversed\"))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "---\n",
        "title: \"ML Methods\"\n",
        "subtitle: \"Predicting Job Posting Duration Using Random Forest Regressor\"\n",
        "author:\n",
        "  - name: Shreya Mani\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this machine learning project, I aimed to predict how long job postings remain active (i.e., their DURATION) using a Random Forest Regressor. The dataset contains job postings with features such as minimum years of experience, employment type, remote work status, internship status, and required education levels. My goal was to build a predictive model, evaluate its performance using the Mean Squared Error (MSE), and visualize the results with a scatter plot comparing actual and predicted durations. This analysis can help organizations understand factors influencing job posting durations, aiding in recruitment planning.\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "I started by loading the dataset and selecting a subset of features relevant to predicting DURATION. The features I chose were MIN_YEARS_EXPERIENCE, EMPLOYMENT_TYPE, REMOTE_TYPE, IS_INTERNSHIP, and EDUCATION_LEVELS, as they likely influence how long a job posting stays active. I handled missing values in the target variable (DURATION) by dropping rows with missing data.\n",
        "\n",
        "A challenge arose with the EDUCATION_LEVELS column, which contained string representations of lists (e.g., '[\\n 2\\n]'). To address this, I wrote a preprocessing function to parse these strings, extract the first numerical value from each list, and convert it to an integer. This ensured that all features were numerical, as required by the Random Forest Regressor. The dataset was then split into training (80%) and testing (20%) sets to evaluate the model's performance on unseen data.\n",
        "\n",
        "Here’s the Python code I used for data preprocessing:\n",
        "```{python}\n",
        "#| label: data-cleaning\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "#| message: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import ast\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Auto-download CSV if missing\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"{csv_path} not found! Attempting to download...\")\n",
        "\n",
        "    os.makedirs('region_analysis', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "\n",
        "    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # <--- your actual file ID\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, csv_path, quiet=False)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"{csv_path} found. Proceeding...\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "df.head()\n",
        "df.tail()"
      ],
      "id": "f5b565de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset (using the provided sample data)\n",
        "data = {\n",
        "    'DURATION': [6.0, np.nan, 35.0, 48.0],\n",
        "    'MIN_YEARS_EXPERIENCE': [2.0, 3.0, 5.0, 3.0],\n",
        "    'EMPLOYMENT_TYPE': [1.0, 1.0, 1.0, 1.0],\n",
        "    'REMOTE_TYPE': [0.0, 1.0, 0.0, 0.0],\n",
        "    'IS_INTERNSHIP': [False, False, False, False],\n",
        "    'EDUCATION_LEVELS': ['[\\n 2\\n]', '[\\n 99\\n]', '[\\n 2\\n]', '[\\n 99\\n]']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Function to parse EDUCATION_LEVELS strings and handle different types\n",
        "def parse_education_levels(edu):\n",
        "    # If the value is already a float, use it directly (or convert to nan if invalid)\n",
        "    if isinstance(edu, float):\n",
        "        return edu if not np.isnan(edu) else np.nan\n",
        "    # If the value is a string, parse it\n",
        "    if isinstance(edu, str):\n",
        "        try:\n",
        "            # Parse the string into a list using ast.literal_eval\n",
        "            edu_list = ast.literal_eval(edu.replace('\\n', ''))\n",
        "            # Return the first numerical value (as an integer)\n",
        "            return int(edu_list[0])\n",
        "        except (ValueError, SyntaxError, IndexError):\n",
        "            return np.nan  # Return NaN if parsing fails\n",
        "    # If the value is neither a string nor a float, return NaN\n",
        "    return np.nan\n",
        "\n",
        "# Apply the parsing function to EDUCATION_LEVELS\n",
        "df['EDUCATION_LEVELS'] = df['EDUCATION_LEVELS'].apply(parse_education_levels)\n",
        "\n",
        "# Drop rows with missing values in DURATION or EDUCATION_LEVELS\n",
        "df = df.dropna(subset=['DURATION', 'EDUCATION_LEVELS'])\n",
        "\n",
        "# Features and target\n",
        "X = df[['MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'IS_INTERNSHIP', 'EDUCATION_LEVELS']]\n",
        "y = df['DURATION']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Testing set size:\", X_test.shape)\n",
        "print(\"Sample of preprocessed EDUCATION_LEVELS:\", df['EDUCATION_LEVELS'].head().tolist())"
      ],
      "id": "fd8e9ce1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training\n",
        "\n",
        "With the data preprocessed, I trained a Random Forest Regressor, a robust model that handles numerical features well and is less prone to overfitting. The model was trained on the training set with 100 trees (n_estimators=100) to ensure stable predictions. Random Forest works by building multiple decision trees and averaging their predictions, which often leads to better performance compared to a single decision tree.\n",
        "\n",
        "Here’s the code for training the Random Forest Regressor:"
      ],
      "id": "201221db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")"
      ],
      "id": "65d8ffc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Visualization\n",
        "After training the model, I used it to predict the DURATION for the test set. To evaluate the model's performance, I calculated the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values. A lower MSE indicates better predictive accuracy.\n",
        "\n",
        "I also created a scatter plot to visualize the model's performance, comparing the actual DURATION values to the predicted ones. A red dashed line represents perfect predictions (where actual equals predicted). Points closer to this line indicate better predictions.\n",
        "\n",
        "Here’s the code for evaluation and visualization:"
      ],
      "id": "2ac08bae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Plot actual vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
        "plt.xlabel('Actual Duration (Days)')\n",
        "plt.ylabel('Predicted Duration (Days)')\n",
        "plt.title('Random Forest Regressor: Actual vs Predicted Job Posting Duration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('actual_vs_predicted_duration.png')"
      ],
      "id": "2b6a88c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "\n",
        "The Mean Squared Error (MSE) provides a quantitative measure of the model's performance. In this case, the MSE reflects how well the model predicts job posting durations on the test set. The scatter plot (actual_vs_predicted_duration.png) visually demonstrates the model's accuracy. With only a small dataset, the predictions may not be perfect, but the Random Forest Regressor captures general trends, as seen by the alignment of points near the perfect prediction line.\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Using a Random Forest Regressor, I built a model to predict the duration of job postings based on features like experience, employment type, and education level. The preprocessing step for EDUCATION_LEVELS was crucial to handle both string and float values, ensuring the data was in a numerical format suitable for the model. However, the model's performance was poor, with an MSE of 1296.00 and a significant overprediction (42.0 days predicted vs. 6.0 days actual), as shown in the scatter plot. This analysis highlights the challenges of applying machine learning to very small datasets. Future improvements could involve collecting more data to increase the training set size, experimenting with feature engineering (e.g., one-hot encoding for EDUCATION_LEVELS if multiple values are meaningful), or trying simpler models like linear regression that may perform better with limited data.\n"
      ],
      "id": "04ef87f9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NLP Methods\"\n",
        "subtitle: \"NLP Analysis: Extracting Required Skills from Job Postings\"\n",
        "author:\n",
        "  - name: Shreya Mani\n",
        "    affiliations:\n",
        "      - id: bu\n",
        "        name: Boston University\n",
        "        city: Boston\n",
        "        state: MA\n",
        "format: \n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    df-print: paged\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "In this project, we used Natural Language Processing (NLP) to extract required skills from job postings based on their description text in the BODY column. The dataset contains job postings with unstructured text descriptions, which often mention skills needed for the role (e.g., \"analyze data\" or \"develop software\"). Our goal was to identify and analyze the most common skills mentioned in these postings, providing insights into the skills most in demand. This analysis can help job seekers understand the key skills to develop and assist employers in identifying trends in skill requirements.\n",
        "\n",
        "# Data Preprocessing\n",
        "We started by loading the dataset and focusing on the BODY column, which contains the job description text. The BODY text is unstructured and requires preprocessing for NLP analysis. We performed the following steps:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tokenization and Cleaning: Converted the text to lowercase, removed punctuation, and tokenized the text into words.\n",
        "\n",
        "\n",
        "\n",
        "Stop Word Removal: Removed common stop words (e.g., \"the\", \"is\") that don’t add meaningful information. We used a predefined list of common stop words to avoid external dependencies.\n",
        "\n",
        "\n",
        "\n",
        "Skill Extraction: Defined a list of common skills relevant to job postings (e.g., \"data analysis,\" \"software development\") and searched for these skills in the cleaned text. For simplicity, we used keyword matching to identify skills, but this could be extended with more advanced NLP techniques like named entity recognition (NER) or pre-trained models.\n",
        "\n",
        "Since this task is exploratory and doesn’t require a target variable, we didn’t split the data into training and testing sets. Instead, we processed all available job descriptions to extract and analyze skills.\n",
        "\n",
        "Here’s the Python code we used for data preprocessing and skill extraction:"
      ],
      "id": "952677e3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: data-cleaning\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import ast\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Auto-download CSV if missing\n",
        "csv_path = 'region_analysis/lightcast_job_postings.csv'\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"{csv_path} not found! Attempting to download...\")\n",
        "\n",
        "    os.makedirs('region_analysis', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        !pip install gdown\n",
        "        import gdown\n",
        "\n",
        "    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # <--- your actual file ID\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, csv_path, quiet=False)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"{csv_path} found. Proceeding...\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('region_analysis/lightcast_job_postings.csv')\n",
        "df.head()\n",
        "df.tail()"
      ],
      "id": "data-cleaning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset (using the provided sample data)\n",
        "data = {\n",
        "    'BODY': [\n",
        "        \"Enterprise Analyst (II-III)\\n\\nRemote work available\\nAnalyze data and provide insights.\",\n",
        "        \"Software Engineer\\n\\nOn-site position in New York\\nDevelop and maintain software systems.\",\n",
        "        np.nan,\n",
        "        \"Data Scientist\\n\\nHybrid role\\nWork with large datasets to build models.\"\n",
        "    ],\n",
        "    'REMOTE_TYPE': [1.0, 0.0, np.nan, 0.0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Drop rows with missing values in BODY\n",
        "df = df.dropna(subset=['BODY'])\n",
        "\n",
        "# Define a list of common stop words (hardcoded to avoid any external dependency)\n",
        "stop_words = {\n",
        "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
        "    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will',\n",
        "    'with', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n",
        "    'yours', 'yourself', 'yourselves', 'him', 'his', 'her', 'hers', 'herself', 'it',\n",
        "    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
        "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
        "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
        "    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
        "    'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
        "    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
        "    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
        "    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
        "}\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize and remove stop words\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply text cleaning\n",
        "df['BODY_CLEANED'] = df['BODY'].apply(clean_text)\n",
        "# Apply text cleaning and print cleaned text to check\n",
        "df['BODY_CLEANED'] = df['BODY'].apply(clean_text)\n",
        "print(\"Cleaned Text Sample:\")\n",
        "print(df[['BODY', 'BODY_CLEANED']].head())\n",
        "\n",
        "# Define a list of common skills to search for\n",
        "skills_list = [\n",
        "    \"data analysis\", \"software development\", \"machine learning\",\n",
        "    \"project management\", \"communication\", \"teamwork\",\n",
        "    \"sql\", \"python\", \"modeling\", \"analytics\"\n",
        "]\n",
        "\n",
        "# Function to extract skills from text\n",
        "def extract_skills(text):\n",
        "    found_skills = []\n",
        "    for skill in skills_list:\n",
        "        # Check if the skill (or a variation) is in the text\n",
        "        if skill in text:\n",
        "            found_skills.append(skill)\n",
        "        # Handle multi-word skills by checking individual words\n",
        "        elif all(word in text.split() for word in skill.split()):\n",
        "            found_skills.append(skill)\n",
        "    return found_skills\n",
        "\n",
        "# Apply skill extraction\n",
        "df['SKILLS'] = df['BODY_CLEANED'].apply(extract_skills)\n",
        "\n",
        "# Flatten the list of skills and count their frequency\n",
        "all_skills = [skill for sublist in df['SKILLS'] for skill in sublist]\n",
        "skill_counts = Counter(all_skills)\n",
        "\n",
        "print(\"Extracted skills and their frequencies:\", dict(skill_counts))\n",
        "print(\"Sample of cleaned text and extracted skills:\")\n",
        "for i in range(len(df)):\n",
        "    print(f\"Job {i+1}: {df['BODY_CLEANED'].iloc[i]} -> Skills: {df['SKILLS'].iloc[i]}\")\n",
        "    # 1. Cleaned text preprocessing (you already have this)\n",
        "df['BODY_CLEANED'] = df['BODY'].apply(clean_text)\n",
        "\n",
        "# 2. Skill extraction (new or updated code here)\n",
        "def extract_skills(text):\n",
        "    found_skills = []\n",
        "    for skill in skills_list:\n",
        "        if skill in text:\n",
        "            found_skills.append(skill)\n",
        "        elif all(word in text.split() for word in skill.split()):\n",
        "            found_skills.append(skill)\n",
        "    return found_skills\n",
        "\n",
        "df['SKILLS'] = df['BODY_CLEANED'].apply(extract_skills)\n",
        "\n",
        "# 3. Visualization of most common skills (you already have this)\n",
        "all_skills = [skill for sublist in df['SKILLS'] for skill in sublist]\n",
        "skill_counts = Counter(all_skills)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "if skill_counts:\n",
        "    skills, counts = zip(*sorted(skill_counts.items(), key=lambda x: x[1], reverse=True))\n",
        "    plt.bar(skills, counts, color='skyblue')\n",
        "    plt.xlabel('Skills')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Most Common Skills in Job Postings')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('skills_frequency.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No skills were found in the job descriptions.\")"
      ],
      "id": "29318656",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skill Analysis and Visualization\n",
        "\n",
        "After extracting skills from the job descriptions, we analyzed their frequency to identify the most common skills mentioned. We visualized the results using a bar plot, showing the count of each skill across all job postings. This helps highlight the skills that are most in demand based on the dataset.\n",
        "\n",
        "Here’s the code we used for analyzing and visualizing the skills:"
      ],
      "id": "4d83cdb9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the most common skills\n",
        "plt.figure(figsize=(10, 6))\n",
        "skills, counts = zip(*skill_counts.items()) if skill_counts else ([], [])\n",
        "if counts:  # Ensure there are skills to plot\n",
        "    plt.bar(skills, counts, color='skyblue')\n",
        "    plt.xlabel('Skills')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Most Common Skills in Job Postings')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('skills_frequency.png')\n",
        "else:\n",
        "    print(\"No skills were found in the job descriptions.\")"
      ],
      "id": "3a545faa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "8f58900e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"References\"\n",
        "---\n",
        "\n",
        "## Predictive Salary Modelling: Leveraging Data Science Skills and Machine Learning for Accurate Forecasting\n",
        "Haseeb, M. A., Viswanathan, R., Iyer, K., Hota, A. R., & Prathaban, B. P. (2024). Predictive salary modelling: Leveraging data science skills and machine learning for accurate forecasting. 2024 9th International Conference on Communication and Electronics Systems (ICCES), 1011–1019. https://ieeexplore.ieee.org/document/10859447\n",
        "\n",
        "## Tackling Economic Inequalities Through Business Analytics: A Literature Review\n",
        "Adaga, E. M., Egieya, Z. E., Ewuga, S. K., Abdul, A. A., & Abrahams, O. (2024). Tackling economic inequalities through business analytics: A literature review. Computer Science & IT Research Journal, 5(1), 60–80. https://doi.org/10.51594/csitjr.v5i1.702\n",
        "\n",
        "## Antecedent Configurations Toward Supply Chain Resilience: The Joint Impact of Supply Chain Integration and Big Data Analytics Capability\n",
        "Jiang, Y., Feng, T., & Huang, Y. (2024). Antecedent configurations toward supply chain resilience: The joint impact of supply chain integration and big data analytics capability. Journal of Operations Management, 70(2), 257–284. https://doi.org/10.1002/joom.1282\n",
        "\n",
        "## Leveraging AI and Data Analytics for Enhancing Financial Inclusion in Developing Economies\n",
        "Adeoye, O. B., Addy, W. A., Ajayi-Nifise, A. O., Odeyemi, O., Okoye, C. C., & Ofodile, O. C. (n.d.). Leveraging AI and data analytics for enhancing financial inclusion in developing economies. Finance & Accounting Research Journal. https://fepbl.com/index.php/farj/article/view/856\n",
        "\n",
        "## Employee Career Decision Making: The Influence of Salary and Benefits, Work Environment and Job Security\n",
        "Achim, N., Badrolhisam, N. I., & Zulkipli, N. (2019). Employee career decision making: The influence of salary and benefits, work environment and job security. Journal of Academia, 7(Special Issue 1), 41–50.\n",
        "\n",
        "## The Influence of Salaries and “Opportunity Costs” on Teachers’ Career Choices\n",
        "Murnane, R. J., Singer, J. D., & Willett, J. B. (1989). The influences of salaries and “opportunity costs” on teachers' career choices: Evidence from North Carolina. Harvard Educational Review, 59(3), 325–349.\n",
        "\n",
        "## The Future of Work: Impacts of AI on Employment and Job Market Dynamics\n",
        "Tomar, A., Sharma, S., Arti, & Suman, S. (2024). The future of work: Impacts of AI on employment and job market dynamics. 2024 International Conference on Progressive Innovations in Intelligent Systems and Data Science (ICPIDS).\n",
        "\n",
        "## AI and Job Market: Analysing the Potential Impact of AI on Employment, Skills, and Job Displacement\n",
        "Faluyi, S. E. (2025). AI and job market: Analysing the potential impact of AI on employment, skills, and job displacement. African Journal of Marketing Management, 17(1), 1–8.\n"
      ],
      "id": "f10485cc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Library/Frameworks/Python.framework/Versions/3.13/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}