{
  "hash": "a313e4f9d05c7a4ad574644bc5d90f86",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NLP Methods\"\nsubtitle: \"NLP Analysis: Extracting Required Skills from Job Postings\"\nauthor:\n  - name: Shreya Mani\n    affiliations:\n      - id: bu\n        name: Boston University\n        city: Boston\n        state: MA\nformat: \n  html:\n        toc: true\n        number-sections: true\n        df-print: paged\n---\n\n\n\n\n# Introduction\nIn this project, we used Natural Language Processing (NLP) to extract required skills from job postings based on their description text in the BODY column. The dataset contains job postings with unstructured text descriptions, which often mention skills needed for the role (e.g., \"analyze data\" or \"develop software\"). Our goal was to identify and analyze the most common skills mentioned in these postings, providing insights into the skills most in demand. This analysis can help job seekers understand the key skills to develop and assist employers in identifying trends in skill requirements.\n\n# Data Preprocessing\nWe started by loading the dataset and focusing on the BODY column, which contains the job description text. The BODY text is unstructured and requires preprocessing for NLP analysis. We performed the following steps:\n\nTokenization and Cleaning: Converted the text to lowercase, removed punctuation, and tokenized the text into words.\n\n\nStop Word Removal: Removed common stop words (e.g., \"the\", \"is\") that don’t add meaningful information. We used a predefined list of common stop words to avoid external dependencies.\n\n\nSkill Extraction: Defined a list of common skills relevant to job postings (e.g., \"data analysis,\" \"software development\") and searched for these skills in the cleaned text. For simplicity, we used keyword matching to identify skills, but this could be extended with more advanced NLP techniques like named entity recognition (NER) or pre-trained models.\n\nSince this task is exploratory and doesn’t require a target variable, we didn’t split the data into training and testing sets. Instead, we processed all available job descriptions to extract and analyze skills.\n\nHere’s the Python code we used for data preprocessing and skill extraction:\n\n::: {#cell-skill-extraction .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nfrom collections import Counter\n\n# Load dataset\ncsv_path = 'region_analysis/lightcast_job_postings.csv'\ndf = pd.read_csv(csv_path)\n\n# Drop rows with missing job descriptions\ndf = df.dropna(subset=['BODY'])\n\n# Define stop words\nstop_words = {\n    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will',\n    'with', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',\n    'yours', 'yourself', 'yourselves', 'him', 'his', 'her', 'hers', 'herself', 'it',\n    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n    'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',\n    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n}\n\n# Clean text\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    tokens = text.split()\n    return ' '.join([word for word in tokens if word not in stop_words])\n\ndf['BODY_CLEANED'] = df['BODY'].apply(clean_text)\n\n# Define common skills\nskills_list = [\n    \"data analysis\", \"software development\", \"machine learning\",\n    \"project management\", \"communication\", \"teamwork\",\n    \"sql\", \"python\", \"modeling\", \"analytics\"\n]\n\n# Extract skills\ndef extract_skills(text):\n    found_skills = []\n    for skill in skills_list:\n        if skill in text:\n            found_skills.append(skill)\n    return found_skills\n\ndf['SKILLS'] = df['BODY_CLEANED'].apply(extract_skills)\n\n# Flatten skill list and count\nall_skills = [skill for sublist in df['SKILLS'] for skill in sublist]\nskill_counts = Counter(all_skills)\n\n\n\n# Visualize top skills\nplt.figure(figsize=(10, 6))\nif skill_counts:\n    skills, counts = zip(*sorted(skill_counts.items(), key=lambda x: x[1], reverse=True))\n    plt.bar(skills, counts, color='skyblue')\n    plt.xlabel('Skills')\n    plt.ylabel('Frequency')\n    plt.title('Most Common Skills in Job Postings')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig('skills_frequency.png')\n    plt.show()\nelse:\n    print(\"No skills were found in the job descriptions.\")\n```\n\n::: {.cell-output .cell-output-display}\n![](nlp_method_files/figure-html/skill-extraction-output-1.png){#skill-extraction}\n:::\n:::\n\n\n# Skill Analysis and Visualization\n\nAfter extracting skills from the job descriptions, we analyzed their frequency to identify the most common skills mentioned. We visualized the results using a bar plot, showing the count of each skill across all job postings. This helps highlight the skills that are most in demand based on the dataset.\n\nHere’s the code we used for analyzing and visualizing the skills:\n\n::: {#048c5af7 .cell execution_count=2}\n``` {.python .cell-code}\n# Plot the most common skills\nplt.figure(figsize=(10, 6))\nskills, counts = zip(*skill_counts.items()) if skill_counts else ([], [])\nif counts:  # Ensure there are skills to plot\n    plt.bar(skills, counts, color='skyblue')\n    plt.xlabel('Skills')\n    plt.ylabel('Frequency')\n    plt.title('Most Common Skills in Job Postings')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig('skills_frequency.png')\nelse:\n    print(\"No skills were found in the job descriptions.\")\n```\n\n::: {.cell-output .cell-output-display}\n![](nlp_method_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "nlp_method_files"
    ],
    "filters": [],
    "includes": {}
  }
}