{
  "hash": "2804a9f953e19022e42d0409bb505518",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ML Methods\"\nsubtitle: \"Predicting Job Posting Duration Using Random Forest Regressor\"\nauthor:\n  - name: \"Shreya Mani\"\n    affiliations:\n          - id: bu\n            name: Boston University\n            city: Boston\n            state: MA\nformat: \n  html:\n        toc: true\n        number-sections: true\n        df-print: paged\n---\n\n\n\n# Introduction\n\nIn this machine learning project, I aimed to predict how long job postings remain active (i.e., their DURATION) using a Random Forest Regressor. The dataset contains job postings with features such as minimum years of experience, employment type, remote work status, internship status, and required education levels. My goal was to build a predictive model, evaluate its performance using the Mean Squared Error (MSE), and visualize the results with a scatter plot comparing actual and predicted durations. This analysis can help organizations understand factors influencing job posting durations, aiding in recruitment planning.\n\n# Data Preprocessing\n\nI started by loading the dataset and selecting a subset of features relevant to predicting DURATION. The features I chose were MIN_YEARS_EXPERIENCE, EMPLOYMENT_TYPE, REMOTE_TYPE, IS_INTERNSHIP, and EDUCATION_LEVELS, as they likely influence how long a job posting stays active. I handled missing values in the target variable (DURATION) by dropping rows with missing data.\n\nA challenge arose with the EDUCATION_LEVELS column, which contained string representations of lists . To address this, I wrote a preprocessing function to parse these strings, extract the first numerical value from each list, and convert it to an integer. This ensured that all features were numerical, as required by the Random Forest Regressor. The dataset was then split into training (80%) and testing (20%) sets to evaluate the model performance on unseen data.\n\nHere the Python code I used for data preprocessing:\n\n::: {#ml-data-cleaning .cell message='false' execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport ast\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Auto-download CSV if missing\ncsv_path = 'region_analysis/lightcast_job_postings.csv'\nif not os.path.exists(csv_path):\n    print(f\"{csv_path} not found! Attempting to download...\")\n    os.makedirs('region_analysis', exist_ok=True)\n    try:\n        import gdown\n    except ImportError:\n        print(\"Installing gdown...\")\n        !pip install gdown\n        import gdown\n    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # Replace with actual file ID\n    url = f'https://drive.google.com/uc?id={file_id}'\n    try:\n        gdown.download(url, csv_path, quiet=False)\n        print(\"Download complete!\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        raise\nelse:\n    print(f\"{csv_path} found. Proceeding...\")\n\n# Load the dataset\ntry:\n    df = pd.read_csv(csv_path)\n    print(f\"Initial dataset size: {df.shape}\")\n    print(f\"Missing values:\\n{df[['DURATION', 'MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'MIN_EDULEVELS']].isnull().sum()}\")\nexcept Exception as e:\n    print(f\"Error loading CSV: {e}\")\n    raise\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nregion_analysis/lightcast_job_postings.csv found. Proceeding...\nInitial dataset size: (72498, 131)\nMissing values:\nDURATION                27316\nMIN_YEARS_EXPERIENCE    23146\nEMPLOYMENT_TYPE            44\nREMOTE_TYPE                44\nMIN_EDULEVELS              44\ndtype: int64\n```\n:::\n:::\n\n\n## Sample of preprocessed EDUCATION_LEVELS\n\n::: {#5610b7a7 .cell execution_count=2}\n``` {.python .cell-code}\n# Define function to parse MIN_EDULEVELS strings\ndef parse_education_levels(edu):\n    if isinstance(edu, (int, float)) and not np.isnan(edu):\n        return int(edu)  # Return integer if already numerical\n    if isinstance(edu, str):\n        try:\n            edu_list = ast.literal_eval(edu.replace('\\n', ''))\n            return int(edu_list[0]) if isinstance(edu_list[0], (int, float)) else np.nan\n        except (ValueError, SyntaxError, IndexError) as e:\n            print(f\"Parsing failed for: {edu}, Error: {e}\")\n            return np.nan\n    return np.nan\n\n# Select features and target\nfeatures = ['MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'IS_INTERNSHIP', 'MIN_EDULEVELS']\ntarget = 'DURATION'\n\n# Check if all features and target exist\nmissing_cols = [col for col in features + [target] if col not in df.columns]\nif missing_cols:\n    print(f\"Missing columns: {missing_cols}\")\n    # If IS_INTERNSHIP is missing, remove it from features\n    if 'IS_INTERNSHIP' in missing_cols:\n        features.remove('IS_INTERNSHIP')\n    else:\n        raise ValueError(\"Required columns not found in dataset\")\n\n# Create a copy of the dataset with selected columns\ndf_subset = df[features + [target]].copy()\n\n# Parse MIN_EDULEVELS\ndf_subset['MIN_EDULEVELS'] = df_subset['MIN_EDULEVELS'].apply(parse_education_levels)\n\n# Handle missing values with imputation for all numerical columns\nnum_cols = [col for col in features if col in df_subset.columns]\nnum_imputer = SimpleImputer(strategy='median')\ndf_subset[num_cols] = num_imputer.fit_transform(df_subset[num_cols])\n\n# Impute DURATION with mean\ndf_subset['DURATION'] = df_subset['DURATION'].fillna(df_subset['DURATION'].mean())\n\n# Ensure IS_INTERNSHIP is integer if present\nif 'IS_INTERNSHIP' in df_subset.columns:\n    df_subset['IS_INTERNSHIP'] = df_subset['IS_INTERNSHIP'].astype(int)\n\n# Verify no missing values\nprint(f\"Missing values after imputation:\\n{df_subset.isnull().sum()}\")\nprint(f\"Preprocessed dataset size: {df_subset.shape}\")\n\n# Features and target\nX = df_subset[num_cols]\ny = df_subset['DURATION']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training set size:\", X_train.shape)\nprint(\"Testing set size:\", X_test.shape)\nprint(\"Sample of preprocessed MIN_EDULEVELS:\", df_subset['MIN_EDULEVELS'].head().tolist())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMissing values after imputation:\nMIN_YEARS_EXPERIENCE    0\nEMPLOYMENT_TYPE         0\nREMOTE_TYPE             0\nIS_INTERNSHIP           0\nMIN_EDULEVELS           0\nDURATION                0\ndtype: int64\nPreprocessed dataset size: (72498, 6)\nTraining set size: (57998, 5)\nTesting set size: (14500, 5)\nSample of preprocessed MIN_EDULEVELS: [2.0, 99.0, 2.0, 99.0, 99.0]\n```\n:::\n:::\n\n\n# Model Training\n\nWith the data preprocessed, I trained a Random Forest Regressor, a robust model that handles numerical features well and is less prone to overfitting. The model was trained on the training set with 100 trees (n_estimators=100) to ensure stable predictions. Random Forest works by building multiple decision trees and averaging their predictions, which often leads to better performance compared to a single decision tree.\n\nHere is the code for training the Random Forest Regressor:\n\n::: {#8210d5ed .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train the Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nprint(\"Model training completed.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel training completed.\n```\n:::\n:::\n\n\n# Model Evaluation and Visualization\nAfter training the model, I used it to predict the DURATION for the test set. To evaluate the model performance, I calculated the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values. A lower MSE indicates better predictive accuracy.\n\nI also created a scatter plot to visualize the model performance, comparing the actual DURATION values to the predicted ones. A red dashed line represents perfect predictions (where actual equals predicted). Points closer to this line indicate better predictions.\n\nHere is the code for evaluation and visualization:\n\n::: {#71b375f6 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# Cross-validation\ncv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\ncv_mse = -cv_scores.mean()\ncv_rmse = np.sqrt(cv_mse)\nprint(f\"Cross-validated MSE: {cv_mse:.2f} Â± {cv_scores.std():.2f}\")\nprint(f\"Cross-validated RMSE: {cv_rmse:.2f} days\")\n\n# Predict on test set\ny_pred = rf.predict(X_test)\n\n# Calculate MSE and RMSE on test set\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Test set MSE: {mse:.2f}\")\nprint(f\"Test set RMSE: {rmse:.2f} days\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': num_cols,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Plot actual vs predicted values\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Predicted vs Actual')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\nplt.xlabel('Actual Duration (Days)')\nplt.ylabel('Predicted Duration (Days)')\nplt.title('Random Forest Regressor: Actual vs Predicted Job Posting Duration')\nplt.legend()\nplt.grid(True)\nplt.savefig('actual_vs_predicted_duration.png')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCross-validated MSE: 126.50 Â± 1.80\nCross-validated RMSE: 11.25 days\nTest set MSE: 124.36\nTest set RMSE: 11.15 days\n\nFeature Importance:\n                Feature  Importance\n0  MIN_YEARS_EXPERIENCE    0.356871\n4         MIN_EDULEVELS    0.308861\n2           REMOTE_TYPE    0.181073\n1       EMPLOYMENT_TYPE    0.103772\n3         IS_INTERNSHIP    0.049423\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ml_methods_files/figure-html/cell-5-output-2.png){width=659 height=523}\n:::\n:::\n\n\n# Results\n\nThe Mean Squared Error (MSE) provides a quantitative measure of the model performance. In this case, the MSE reflects how well the model predicts job posting durations on the test set. The scatter plot (actual_vs_predicted_duration.png) visually demonstrates the model accuracy. With only a small dataset, the predictions may not be perfect, but the Random Forest Regressor captures general trends, as seen by the alignment of points near the perfect prediction line.\n\n# Conclusion\n\nUsing a Random Forest Regressor, I built a model to predict the duration of job postings based on features like experience, employment type, and education level. The preprocessing step for EDUCATION_LEVELS was crucial to handle both string and float values, ensuring the data was in a numerical format suitable for the model. However, the model performance was poor, with an MSE of 1296.00 and a significant overprediction (42.0 days predicted vs. 6.0 days actual), as shown in the scatter plot. This analysis highlights the challenges of applying machine learning to very small datasets. Future improvements could involve collecting more data to increase the training set size, experimenting with feature engineering (e.g., one-hot encoding for EDUCATION_LEVELS if multiple values are meaningful), or trying simpler models like linear regression that may perform better with limited data.\n\n",
    "supporting": [
      "ml_methods_files"
    ],
    "filters": [],
    "includes": {}
  }
}