{"title":"ML Methods","markdown":{"yaml":{"title":"ML Methods","subtitle":"Predicting Job Posting Duration Using Random Forest Regressor","author":[{"name":"Shreya Mani","affiliations":[{"id":"bu","name":"Boston University","city":"Boston","state":"MA"}]}],"format":{"html":{"toc":true,"number-sections":true,"df-print":"paged"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nIn this machine learning project, I aimed to predict how long job postings remain active (i.e., their DURATION) using a Random Forest Regressor. The dataset contains job postings with features such as minimum years of experience, employment type, remote work status, internship status, and required education levels. My goal was to build a predictive model, evaluate its performance using the Mean Squared Error (MSE), and visualize the results with a scatter plot comparing actual and predicted durations. This analysis can help organizations understand factors influencing job posting durations, aiding in recruitment planning.\n\n# Data Preprocessing\n\nI started by loading the dataset and selecting a subset of features relevant to predicting DURATION. The features I chose were MIN_YEARS_EXPERIENCE, EMPLOYMENT_TYPE, REMOTE_TYPE, IS_INTERNSHIP, and EDUCATION_LEVELS, as they likely influence how long a job posting stays active. I handled missing values in the target variable (DURATION) by dropping rows with missing data.\n\nA challenge arose with the EDUCATION_LEVELS column, which contained string representations of lists . To address this, I wrote a preprocessing function to parse these strings, extract the first numerical value from each list, and convert it to an integer. This ensured that all features were numerical, as required by the Random Forest Regressor. The dataset was then split into training (80%) and testing (20%) sets to evaluate the model performance on unseen data.\n\nHere the Python code I used for data preprocessing:\n```{python}\n#| label: ml data cleaning\n#| echo: true\n#| warning: false\n#| message: false\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport ast\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Auto-download CSV if missing\ncsv_path = 'region_analysis/lightcast_job_postings.csv'\nif not os.path.exists(csv_path):\n    print(f\"{csv_path} not found! Attempting to download...\")\n    os.makedirs('region_analysis', exist_ok=True)\n    try:\n        import gdown\n    except ImportError:\n        print(\"Installing gdown...\")\n        !pip install gdown\n        import gdown\n    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # Replace with actual file ID\n    url = f'https://drive.google.com/uc?id={file_id}'\n    try:\n        gdown.download(url, csv_path, quiet=False)\n        print(\"Download complete!\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        raise\nelse:\n    print(f\"{csv_path} found. Proceeding...\")\n\n# Load the dataset\ntry:\n    df = pd.read_csv(csv_path)\n    print(f\"Initial dataset size: {df.shape}\")\n    print(f\"Missing values:\\n{df[['DURATION', 'MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'MIN_EDULEVELS']].isnull().sum()}\")\nexcept Exception as e:\n    print(f\"Error loading CSV: {e}\")\n    raise\n```\n\n\n## Sample of preprocessed EDUCATION_LEVELS\n```{python}\n# Define function to parse MIN_EDULEVELS strings\ndef parse_education_levels(edu):\n    if isinstance(edu, (int, float)) and not np.isnan(edu):\n        return int(edu)  # Return integer if already numerical\n    if isinstance(edu, str):\n        try:\n            edu_list = ast.literal_eval(edu.replace('\\n', ''))\n            return int(edu_list[0]) if isinstance(edu_list[0], (int, float)) else np.nan\n        except (ValueError, SyntaxError, IndexError) as e:\n            print(f\"Parsing failed for: {edu}, Error: {e}\")\n            return np.nan\n    return np.nan\n\n# Select features and target\nfeatures = ['MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'IS_INTERNSHIP', 'MIN_EDULEVELS']\ntarget = 'DURATION'\n\n# Check if all features and target exist\nmissing_cols = [col for col in features + [target] if col not in df.columns]\nif missing_cols:\n    print(f\"Missing columns: {missing_cols}\")\n    # If IS_INTERNSHIP is missing, remove it from features\n    if 'IS_INTERNSHIP' in missing_cols:\n        features.remove('IS_INTERNSHIP')\n    else:\n        raise ValueError(\"Required columns not found in dataset\")\n\n# Create a copy of the dataset with selected columns\ndf_subset = df[features + [target]].copy()\n\n# Parse MIN_EDULEVELS\ndf_subset['MIN_EDULEVELS'] = df_subset['MIN_EDULEVELS'].apply(parse_education_levels)\n\n# Handle missing values with imputation for all numerical columns\nnum_cols = [col for col in features if col in df_subset.columns]\nnum_imputer = SimpleImputer(strategy='median')\ndf_subset[num_cols] = num_imputer.fit_transform(df_subset[num_cols])\n\n# Impute DURATION with mean\ndf_subset['DURATION'] = df_subset['DURATION'].fillna(df_subset['DURATION'].mean())\n\n# Ensure IS_INTERNSHIP is integer if present\nif 'IS_INTERNSHIP' in df_subset.columns:\n    df_subset['IS_INTERNSHIP'] = df_subset['IS_INTERNSHIP'].astype(int)\n\n# Verify no missing values\nprint(f\"Missing values after imputation:\\n{df_subset.isnull().sum()}\")\nprint(f\"Preprocessed dataset size: {df_subset.shape}\")\n\n# Features and target\nX = df_subset[num_cols]\ny = df_subset['DURATION']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training set size:\", X_train.shape)\nprint(\"Testing set size:\", X_test.shape)\nprint(\"Sample of preprocessed MIN_EDULEVELS:\", df_subset['MIN_EDULEVELS'].head().tolist())\n```\n\n# Model Training\n\nWith the data preprocessed, I trained a Random Forest Regressor, a robust model that handles numerical features well and is less prone to overfitting. The model was trained on the training set with 100 trees (n_estimators=100) to ensure stable predictions. Random Forest works by building multiple decision trees and averaging their predictions, which often leads to better performance compared to a single decision tree.\n\nHere is the code for training the Random Forest Regressor:\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train the Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nprint(\"Model training completed.\")\n```\n\n# Model Evaluation and Visualization\nAfter training the model, I used it to predict the DURATION for the test set. To evaluate the model performance, I calculated the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values. A lower MSE indicates better predictive accuracy.\n\nI also created a scatter plot to visualize the model performance, comparing the actual DURATION values to the predicted ones. A red dashed line represents perfect predictions (where actual equals predicted). Points closer to this line indicate better predictions.\n\nHere is the code for evaluation and visualization:\n\n```{python}\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# Cross-validation\ncv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\ncv_mse = -cv_scores.mean()\ncv_rmse = np.sqrt(cv_mse)\nprint(f\"Cross-validated MSE: {cv_mse:.2f} Â± {cv_scores.std():.2f}\")\nprint(f\"Cross-validated RMSE: {cv_rmse:.2f} days\")\n\n# Predict on test set\ny_pred = rf.predict(X_test)\n\n# Calculate MSE and RMSE on test set\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Test set MSE: {mse:.2f}\")\nprint(f\"Test set RMSE: {rmse:.2f} days\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': num_cols,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Plot actual vs predicted values\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Predicted vs Actual')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\nplt.xlabel('Actual Duration (Days)')\nplt.ylabel('Predicted Duration (Days)')\nplt.title('Random Forest Regressor: Actual vs Predicted Job Posting Duration')\nplt.legend()\nplt.grid(True)\nplt.savefig('actual_vs_predicted_duration.png')\nplt.show()\n```\n\n# Results\n\nThe Mean Squared Error (MSE) provides a quantitative measure of the model performance. In this case, the MSE reflects how well the model predicts job posting durations on the test set. The scatter plot (actual_vs_predicted_duration.png) visually demonstrates the model accuracy. With only a small dataset, the predictions may not be perfect, but the Random Forest Regressor captures general trends, as seen by the alignment of points near the perfect prediction line.\n\n# Conclusion\n\nUsing a Random Forest Regressor, I built a model to predict the duration of job postings based on features like experience, employment type, and education level. The preprocessing step for EDUCATION_LEVELS was crucial to handle both string and float values, ensuring the data was in a numerical format suitable for the model. However, the model performance was poor, with an MSE of 1296.00 and a significant overprediction (42.0 days predicted vs. 6.0 days actual), as shown in the scatter plot. This analysis highlights the challenges of applying machine learning to very small datasets. Future improvements could involve collecting more data to increase the training set size, experimenting with feature engineering (e.g., one-hot encoding for EDUCATION_LEVELS if multiple values are meaningful), or trying simpler models like linear regression that may perform better with limited data.","srcMarkdownNoYaml":"\n# Introduction\n\nIn this machine learning project, I aimed to predict how long job postings remain active (i.e., their DURATION) using a Random Forest Regressor. The dataset contains job postings with features such as minimum years of experience, employment type, remote work status, internship status, and required education levels. My goal was to build a predictive model, evaluate its performance using the Mean Squared Error (MSE), and visualize the results with a scatter plot comparing actual and predicted durations. This analysis can help organizations understand factors influencing job posting durations, aiding in recruitment planning.\n\n# Data Preprocessing\n\nI started by loading the dataset and selecting a subset of features relevant to predicting DURATION. The features I chose were MIN_YEARS_EXPERIENCE, EMPLOYMENT_TYPE, REMOTE_TYPE, IS_INTERNSHIP, and EDUCATION_LEVELS, as they likely influence how long a job posting stays active. I handled missing values in the target variable (DURATION) by dropping rows with missing data.\n\nA challenge arose with the EDUCATION_LEVELS column, which contained string representations of lists . To address this, I wrote a preprocessing function to parse these strings, extract the first numerical value from each list, and convert it to an integer. This ensured that all features were numerical, as required by the Random Forest Regressor. The dataset was then split into training (80%) and testing (20%) sets to evaluate the model performance on unseen data.\n\nHere the Python code I used for data preprocessing:\n```{python}\n#| label: ml data cleaning\n#| echo: true\n#| warning: false\n#| message: false\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport ast\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Auto-download CSV if missing\ncsv_path = 'region_analysis/lightcast_job_postings.csv'\nif not os.path.exists(csv_path):\n    print(f\"{csv_path} not found! Attempting to download...\")\n    os.makedirs('region_analysis', exist_ok=True)\n    try:\n        import gdown\n    except ImportError:\n        print(\"Installing gdown...\")\n        !pip install gdown\n        import gdown\n    file_id = '1V2GCHGt2dkFGqVBeoUFckU4IhUgk4ocQ'  # Replace with actual file ID\n    url = f'https://drive.google.com/uc?id={file_id}'\n    try:\n        gdown.download(url, csv_path, quiet=False)\n        print(\"Download complete!\")\n    except Exception as e:\n        print(f\"Download failed: {e}\")\n        raise\nelse:\n    print(f\"{csv_path} found. Proceeding...\")\n\n# Load the dataset\ntry:\n    df = pd.read_csv(csv_path)\n    print(f\"Initial dataset size: {df.shape}\")\n    print(f\"Missing values:\\n{df[['DURATION', 'MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'MIN_EDULEVELS']].isnull().sum()}\")\nexcept Exception as e:\n    print(f\"Error loading CSV: {e}\")\n    raise\n```\n\n\n## Sample of preprocessed EDUCATION_LEVELS\n```{python}\n# Define function to parse MIN_EDULEVELS strings\ndef parse_education_levels(edu):\n    if isinstance(edu, (int, float)) and not np.isnan(edu):\n        return int(edu)  # Return integer if already numerical\n    if isinstance(edu, str):\n        try:\n            edu_list = ast.literal_eval(edu.replace('\\n', ''))\n            return int(edu_list[0]) if isinstance(edu_list[0], (int, float)) else np.nan\n        except (ValueError, SyntaxError, IndexError) as e:\n            print(f\"Parsing failed for: {edu}, Error: {e}\")\n            return np.nan\n    return np.nan\n\n# Select features and target\nfeatures = ['MIN_YEARS_EXPERIENCE', 'EMPLOYMENT_TYPE', 'REMOTE_TYPE', 'IS_INTERNSHIP', 'MIN_EDULEVELS']\ntarget = 'DURATION'\n\n# Check if all features and target exist\nmissing_cols = [col for col in features + [target] if col not in df.columns]\nif missing_cols:\n    print(f\"Missing columns: {missing_cols}\")\n    # If IS_INTERNSHIP is missing, remove it from features\n    if 'IS_INTERNSHIP' in missing_cols:\n        features.remove('IS_INTERNSHIP')\n    else:\n        raise ValueError(\"Required columns not found in dataset\")\n\n# Create a copy of the dataset with selected columns\ndf_subset = df[features + [target]].copy()\n\n# Parse MIN_EDULEVELS\ndf_subset['MIN_EDULEVELS'] = df_subset['MIN_EDULEVELS'].apply(parse_education_levels)\n\n# Handle missing values with imputation for all numerical columns\nnum_cols = [col for col in features if col in df_subset.columns]\nnum_imputer = SimpleImputer(strategy='median')\ndf_subset[num_cols] = num_imputer.fit_transform(df_subset[num_cols])\n\n# Impute DURATION with mean\ndf_subset['DURATION'] = df_subset['DURATION'].fillna(df_subset['DURATION'].mean())\n\n# Ensure IS_INTERNSHIP is integer if present\nif 'IS_INTERNSHIP' in df_subset.columns:\n    df_subset['IS_INTERNSHIP'] = df_subset['IS_INTERNSHIP'].astype(int)\n\n# Verify no missing values\nprint(f\"Missing values after imputation:\\n{df_subset.isnull().sum()}\")\nprint(f\"Preprocessed dataset size: {df_subset.shape}\")\n\n# Features and target\nX = df_subset[num_cols]\ny = df_subset['DURATION']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training set size:\", X_train.shape)\nprint(\"Testing set size:\", X_test.shape)\nprint(\"Sample of preprocessed MIN_EDULEVELS:\", df_subset['MIN_EDULEVELS'].head().tolist())\n```\n\n# Model Training\n\nWith the data preprocessed, I trained a Random Forest Regressor, a robust model that handles numerical features well and is less prone to overfitting. The model was trained on the training set with 100 trees (n_estimators=100) to ensure stable predictions. Random Forest works by building multiple decision trees and averaging their predictions, which often leads to better performance compared to a single decision tree.\n\nHere is the code for training the Random Forest Regressor:\n```{python}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train the Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nprint(\"Model training completed.\")\n```\n\n# Model Evaluation and Visualization\nAfter training the model, I used it to predict the DURATION for the test set. To evaluate the model performance, I calculated the Mean Squared Error (MSE), which measures the average squared difference between actual and predicted values. A lower MSE indicates better predictive accuracy.\n\nI also created a scatter plot to visualize the model performance, comparing the actual DURATION values to the predicted ones. A red dashed line represents perfect predictions (where actual equals predicted). Points closer to this line indicate better predictions.\n\nHere is the code for evaluation and visualization:\n\n```{python}\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n# Cross-validation\ncv_scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\ncv_mse = -cv_scores.mean()\ncv_rmse = np.sqrt(cv_mse)\nprint(f\"Cross-validated MSE: {cv_mse:.2f} Â± {cv_scores.std():.2f}\")\nprint(f\"Cross-validated RMSE: {cv_rmse:.2f} days\")\n\n# Predict on test set\ny_pred = rf.predict(X_test)\n\n# Calculate MSE and RMSE on test set\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"Test set MSE: {mse:.2f}\")\nprint(f\"Test set RMSE: {rmse:.2f} days\")\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': num_cols,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)\n\n# Plot actual vs predicted values\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Predicted vs Actual')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\nplt.xlabel('Actual Duration (Days)')\nplt.ylabel('Predicted Duration (Days)')\nplt.title('Random Forest Regressor: Actual vs Predicted Job Posting Duration')\nplt.legend()\nplt.grid(True)\nplt.savefig('actual_vs_predicted_duration.png')\nplt.show()\n```\n\n# Results\n\nThe Mean Squared Error (MSE) provides a quantitative measure of the model performance. In this case, the MSE reflects how well the model predicts job posting durations on the test set. The scatter plot (actual_vs_predicted_duration.png) visually demonstrates the model accuracy. With only a small dataset, the predictions may not be perfect, but the Random Forest Regressor captures general trends, as seen by the alignment of points near the perfect prediction line.\n\n# Conclusion\n\nUsing a Random Forest Regressor, I built a model to predict the duration of job postings based on features like experience, employment type, and education level. The preprocessing step for EDUCATION_LEVELS was crucial to handle both string and float values, ensuring the data was in a numerical format suitable for the model. However, the model performance was poor, with an MSE of 1296.00 and a significant overprediction (42.0 days predicted vs. 6.0 days actual), as shown in the scatter plot. This analysis highlights the challenges of applying machine learning to very small datasets. Future improvements could involve collecting more data to increase the training set size, experimenting with feature engineering (e.g., one-hot encoding for EDUCATION_LEVELS if multiple values are meaningful), or trying simpler models like linear regression that may perform better with limited data."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"number-sections":true,"output-file":"ML.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","theme":"cosmo","title":"ML Methods","subtitle":"Predicting Job Posting Duration Using Random Forest Regressor","author":[{"name":"Shreya Mani","affiliations":[{"id":"bu","name":"Boston University","city":"Boston","state":"MA"}]}]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}