---
title: "NLP Methods"
subtitle: "NLP Analysis: Extracting Required Skills from Job Postings"
author:
  - name: Shreya Mani
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
format: 
  html:
        toc: true
        number-sections: true
        df-print: paged
---

# Introduction
In this project, we used Natural Language Processing (NLP) to extract required skills from job postings based on their description text in the BODY column. The dataset contains job postings with unstructured text descriptions, which often mention skills needed for the role (e.g., "analyze data" or "develop software"). Our goal was to identify and analyze the most common skills mentioned in these postings, providing insights into the skills most in demand. This analysis can help job seekers understand the key skills to develop and assist employers in identifying trends in skill requirements.

# Data Preprocessing
We started by loading the dataset and focusing on the BODY column, which contains the job description text. The BODY text is unstructured and requires preprocessing for NLP analysis. We performed the following steps:

Tokenization and Cleaning: Converted the text to lowercase, removed punctuation, and tokenized the text into words.


Stop Word Removal: Removed common stop words (e.g., "the", "is") that don’t add meaningful information. We used a predefined list of common stop words to avoid external dependencies.


Skill Extraction: Defined a list of common skills relevant to job postings (e.g., "data analysis," "software development") and searched for these skills in the cleaned text. For simplicity, we used keyword matching to identify skills, but this could be extended with more advanced NLP techniques like named entity recognition (NER) or pre-trained models.

Since this task is exploratory and doesn’t require a target variable, we didn’t split the data into training and testing sets. Instead, we processed all available job descriptions to extract and analyze skills.

Here’s the Python code we used for data preprocessing and skill extraction:
```{python}
#| label: skill-extraction
#| echo: true
#| warning: false
#| message: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
from collections import Counter

# Load dataset
csv_path = 'region_analysis/lightcast_job_postings.csv'
df = pd.read_csv(csv_path)

# Drop rows with missing job descriptions
df = df.dropna(subset=['BODY'])

# Define stop words
stop_words = {
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',
    'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will',
    'with', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your',
    'yours', 'yourself', 'yourselves', 'him', 'his', 'her', 'hers', 'herself', 'it',
    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',
    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',
    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',
    'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',
    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',
    'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',
    'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when',
    'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',
    'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',
    's', 't', 'can', 'will', 'just', 'don', 'should', 'now'
}

# Clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    return ' '.join([word for word in tokens if word not in stop_words])

df['BODY_CLEANED'] = df['BODY'].apply(clean_text)

# Define common skills
skills_list = [
    "data analysis", "software development", "machine learning",
    "project management", "communication", "teamwork",
    "sql", "python", "modeling", "analytics"
]

# Extract skills
def extract_skills(text):
    found_skills = []
    for skill in skills_list:
        if skill in text:
            found_skills.append(skill)
    return found_skills

df['SKILLS'] = df['BODY_CLEANED'].apply(extract_skills)

# Flatten skill list and count
all_skills = [skill for sublist in df['SKILLS'] for skill in sublist]
skill_counts = Counter(all_skills)



# Visualize top skills
plt.figure(figsize=(10, 6))
if skill_counts:
    skills, counts = zip(*sorted(skill_counts.items(), key=lambda x: x[1], reverse=True))
    plt.bar(skills, counts, color='skyblue')
    plt.xlabel('Skills')
    plt.ylabel('Frequency')
    plt.title('Most Common Skills in Job Postings')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('skills_frequency.png')
    plt.show()
else:
    print("No skills were found in the job descriptions.")
```
# Skill Analysis and Visualization

After extracting skills from the job descriptions, we analyzed their frequency to identify the most common skills mentioned. We visualized the results using a bar plot, showing the count of each skill across all job postings. This helps highlight the skills that are most in demand based on the dataset.

Here’s the code we used for analyzing and visualizing the skills: